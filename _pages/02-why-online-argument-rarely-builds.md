---
layout: default
title: 2. What’s Avoided at Scale
nav_order: 3
---

## 2. What’s Avoided at Scale

Poet [Andrew Choate](https://andrewchoate.us/) holds up a mirror at an angle that flatters few:

> Is it lazier to make a big thing, make a lot of things, or to make a small thing, not very much?
There’s certainly an aura—perhaps even a suspicion—of laziness in prolificness.
>

Choate’s insight is more than artistic musing—it touches something structural. In this framing, laziness isn’t about the amount of visible effort—how prolific or energetic someone appears—but about the effort that’s systematically avoided: the harder, slower work of cognitive depth—sustained attention to logical consistency, internal synthesis, and ethical reflection. A person who produces constantly is not thinking hard enough.

The concept of *productive laziness* scales perfectly to the Internet, where prolificness dominates—across users who favor immediacy over depth, across platforms that amplify volume over structure, and across time as habits solidify into norms. We churn out tweets, comments, videos, and threads—creative, at times, but rarely cumulative, seldom building upon reasoning[^1]. Head down in sweaty pursuit of algorithmic incentives and social validation, we don’t resist its omission.

Reasoning is expensive biologically[^2]: it draws heavily on focus, memory, and effortful synthesis. What’s avoided depends on the person and context—attention span, cognitive bandwidth, platform norms, and even educational background all play a role. But at scale, the collective result of these savings is telling. Because this cost is high—and because social platforms depend on a constant stream of fast, voluminous human input to survive—these deeper forms of reasoning often get displaced. What’s avoided at scale becomes a structural gap—or more precisely, a thinning of what holds thought together.

Large language models may amplify this tendency. Studies have shown that LLMs, while capable of impressive fluency, frequently reinforce or obscure fallacious reasoning unless deliberately constrained[^3].

And so, the Internet has grown vast and expressive—yet hollowed of connective logic. This didn’t happen by decree—it emerged through a kind of ambient laziness: the systematic deferral of slow, effortful thought.

Into this landscape, FallacyTag enters as a quiet provocation—a proposal to nudge things in the opposite direction. Not by slowing speech, but by making reasoning’s absence—or its fragility—visible: logical structure, causal inference, and ethical coherence. These forms are not only frequently absent; when they do appear, they often lack the scaffolding to support them across context or scale.

FallacyTag aims not just to detect faulty reasoning, but to surface the deeper structures that support reasoning itself—highlighting what holds thought together, not merely what breaks it. Prior research has explored how to model fallacies structurally for NLP tasks[^4]. FallacyTag leans on this insight—but shifts the focus from internal model reasoning to external human visibility. Our goal is to scaffold cumulative reasoning in the wild: to make logic legible not only to machines, but to the people engaging with public thought on media platforms.

----

[^1]: The source uses all three markup languages.
[^2]: The source uses all three markup languages.
[^3]: Studies such as Teo et al. (2025), and Manickavasagam & Bandara (2025), have shown that LLMs, while capable of impressive fluency, frequently reinforce or obscure fallacious reasoning unless deliberately constrained.
[^4]: Lei & Huang’s *Logical Structure Tree* (2024), and the broader *Tree of Fallacies* approach,
