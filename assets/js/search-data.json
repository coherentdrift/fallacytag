{"0": {
    "doc": "Hello",
    "title": "FallacyTag: Surfacing Argument Structure in Public Discourse",
    "content": "A lightweight reasoning-feedback layer for flawed logic in the wild. üì¨ Subscribe on Substack üß™ Live interactive demo . FallacyTag is an experiment in offering structure to public reasoning. It explores how we might give flawed arguments the same visibility as typos‚Äî not to shame, but to support reflection, clarity, and better conversations. This site hosts the live draft of the FallacyTag paper. Sections are being released incrementally. So far, the following are available: . | ‚úÖ 0: Abstract | ‚úÖ 1: Introduction | ‚úÖ 2: The Architecture of Avoidance | ‚úÖ 3: Scaffolded Intelligence: How LLMs Can Support Better Reasoning | ‚úÖ 4: FallacyTag: A Gentle Loop for Sharper Thinking | ‚úÖ 5: Mediums, Modalities and Fit | ‚úÖ 6: How It Works | ‚úÖ 7: Is It Feasible? | ‚úÖ 8: Staying Honest at the Edges | ‚úÖ 9: Conditional Case: When This Is Worth Building | ‚è≥ Section 10: Coming soon | . Check the sidebar for what‚Äôs currently available, or follow the project on Substack for updates. If you‚Äôre a builder, educator, or curious skeptic‚Äîwelcome. ",
    "url": "/fallacytag/#fallacytag-surfacing-argument-structure-in-public-discourse",
    
    "relUrl": "/#fallacytag-surfacing-argument-structure-in-public-discourse"
  },"1": {
    "doc": "Hello",
    "title": "Hello",
    "content": " ",
    "url": "/fallacytag/",
    
    "relUrl": "/"
  },"2": {
    "doc": "0. Abstract",
    "title": "0. Abstract",
    "content": "FallacyTag is a lightweight reasoning aid designed to surface flawed argument structure in online discourse‚Äîgently, interpretably, and without moralizing. Built around modern LLM capabilities and constrained design, it offers sentence-level feedback in the form of informal fallacy cues: not judgments, but structural prompts. This paper presents the concept, sketches its architecture, explores its feasibility across technical, social, and conceptual dimensions, and proposes a framework for deployment in varied modalities (text, audio, video). The core bet is simple: by making reasoning structure visible‚Äîeven imperfectly‚Äîwe can foster better reflection without requiring cultural revolutions or cognitive overhauls. FallacyTag isn‚Äôt a moderator or a fact-checker. It‚Äôs a nudge toward clarity, designed to fit where logic matters and tone is fragile. The paper is exploratory in form, rigorous in ambition, and deliberately uneven in tone. It is meant as a public proposal‚Äîfor critique, adaptation, or reinvention. Next . ",
    "url": "/fallacytag/pages/00-abstract/",
    
    "relUrl": "/pages/00-abstract/"
  },"3": {
    "doc": "1. What You Are Reading",
    "title": "1. What You Are Reading",
    "content": "This is a civic design essay. It‚Äôs not a pitch, white paper, or research submission. It‚Äôs an exploration‚Äîpart concept proposal, part system sketch, part cultural provocation. The core idea is technical: a lightweight, modular tool to highlight flawed reasoning in digital conversation. But the surrounding context is human. Tone, trust, and interpretation matter as much as model performance. So this paper moves between domains‚Äîlanguage models, argument theory, pedagogy, and UX‚Äîand switches tone often. Some sections are nerdy. Others are more playful, skeptical, or intentionally jarring. If that feels uneven, it‚Äôs on purpose. Structural humility isn‚Äôt just a design constraint‚Äîit‚Äôs a writing constraint, too. This is for builders, educators, interface designers, and critics. You don‚Äôt have to agree. But if it makes you pause, test, or reframe, then it‚Äôs doing its job. Previous Next . ",
    "url": "/fallacytag/pages/01-what-you-are-reading/",
    
    "relUrl": "/pages/01-what-you-are-reading/"
  },"4": {
    "doc": "2. The Architecture of Avoidance",
    "title": "2. The Architecture of Avoidance",
    "content": " ",
    "url": "/fallacytag/pages/02-the-architecture-of-avoidance/",
    
    "relUrl": "/pages/02-the-architecture-of-avoidance/"
  },"5": {
    "doc": "2. The Architecture of Avoidance",
    "title": "Table of Contents",
    "content": ". | 2.0 What‚Äôs Avoided at Scale | 2.1. The Decline Question | 2.2 A Fork in the Feedback | . 2.0 What‚Äôs Avoided at Scale . Poet Andrew Choate holds up a mirror at an angle that flatters few: . Is it lazier to make a big thing, make a lot of things, or to make a small thing, not very much? There‚Äôs certainly an aura‚Äîperhaps even a suspicion‚Äîof laziness in prolificness. Choate‚Äôs insight is more than artistic musing‚Äîit touches something structural. In this framing, laziness isn‚Äôt about the amount of visible effort‚Äîhow prolific or energetic someone appears‚Äîbut about the effort that‚Äôs systematically avoided: the harder, slower work of cognitive depth‚Äîlike tracking internal consistency across ideas, integrating perspectives, or wrestling with moral consequences. A person who constantly produces is not thinking hard enough. Reasoning is expensive biologically1: it draws heavily on focus, memory, and effortful synthesis. What‚Äôs avoided depends on the person and context‚Äîattention span, cognitive bandwidth, platform norms, and even educational background. People think just well enough to get by most of the time, which is to say, we don‚Äôt think well often. In the digital realm, the collective result of these savings is hard to miss even for a budget observer. The concept of¬†productive laziness¬†scales perfectly to the Internet, where prolificness dominates‚Äîacross users who favor immediacy over depth, across platforms that amplify volume over structure, and across time as habits solidify into norms. We churn out tweets, comments, videos, and threads‚Äîcreative, at times, but rarely cumulative. They seldom connect, challenge, or extend ideas in ways that build reasoning2. Entranced in sweaty pursuit of algorithmic incentives and social validation, we don‚Äôt resist the omission. Because reasoning is costly and platforms are optimized for metabolic thrift (privileging ease, speed, and volume), deeper forms of thought get displaced. Individual shortcuts are just that, but how the system responds to them invites scrutiny. The more we reward rapid, prolific contributions, the more those patterns dominate‚Äîand the more algorithms tune themselves to those patterns in turn. Poor reasoning isn‚Äôt just common; it‚Äôs reinforced. A positive feedback loop emerges: lazy thinking gets surfaced, incentivized, copied, and rewarded, until it becomes the baseline rhythm of public discourse. What‚Äôs avoided at scale becomes not just a personal lapse, but a structural erosion‚Äîa thinning of what holds thought together. Like a microphone too close to a speaker, the system begins to amplify its distortion. flowchart TD U[User Input: Quick, Shallow Content] P[Platform Incentives: Likes, Shares, Visibility] A[Algorithmic Prioritization: Surface &amp; Amplify] N[Norm Formation: Prolificness = Success] B[User Behavior Shifts: More Shortcut Thinking] U --&gt; P --&gt; A --&gt; N --&gt; B --&gt; U classDef loop fill:#fffbe6,stroke:#c9a100,stroke-width:2px; class U,P,A,N,B loop . Figure: The Reasoning Feedback Loop. And so, the Internet has grown vast and expressive‚Äîyet hollowed of connective logic3. This didn‚Äôt happen by decree‚Äîit emerged through a kind of ambient laziness: the positive feedback loop of systematic deferral of slow, effortful thought. 2.1. The Decline Question . We are taking a stroll among big ideas and dramatic questions, and here it‚Äôs paramount to walk past the wrong ones. I am reminded of a story that the legendary composer John Cage told in Indeterminacy: . After the concert I was walking along with the composer and he was telling me how the performances had not been quite up to snuff. So I said, ‚ÄúWell, I enjoyed the music, but I didn‚Äôt agree with that program note about there being too much pain in the world. He said, ‚ÄúWhat? Don‚Äôt you think there‚Äôs enough?‚Äù I said, ‚ÄúI think there‚Äôs just the right amount‚Äù. Cage‚Äôs provocation points out that saying ‚Äútoo much suffering‚Äù begs the question: relative to what ideal, what baseline, and by what mechanism are we measuring? This is precisely the kind of ambiguity that should give us pause before declaring reasoning in decline, whether due to platforms or anything else. Many critiques of modern discourse, from social media to political polarization, carry a background hum of nostalgia: the sense that people used to think better. But as a diagnostic, it‚Äôs deeply fraught. Measuring ‚Äúreasoning‚Äù is hard enough. Comparing it across platforms, eras, and populations is harder still. What‚Äôs the unit of analysis? A syllogism? A thread? A voter base? A classroom? Should we weigh visibility, frequency, coherence, and influence? There‚Äôs no natural denominator‚Äîand no stable point of reference. Worse, the question may not even be consequential. Suppose we knew that the average quality of public reasoning had declined. What exactly would follow? Our task would still be to improve it. Or suppose reasoning had increased, but become less discoverable amid digital noise. That, too, would call for new tools and better infrastructure. We are looking to side-step the irony of dedicating the previous chapter to the sorry state of the online discourse, and avoid the descent into helpless lament. Rather than ask whether reasoning has declined, we prefer to ask: how can we support more of it, in more places, with less friction and more uptake? . We‚Äôre dissatisfied with the current reasoning environment, but we also see in it a challenge that lends itself, at least in part, to architectural solutions. 2.2 A Fork in the Feedback . The rise of large language models adds a new layer of complexity to the landscape we‚Äôve just described. LLMs are trained on the same public discourse that platforms already distort‚Äîlanguage shaped by speed, ambiguity, persuasion, and prolificness. So it‚Äôs reasonable to worry that they might amplify the positive feedback loop we‚Äôve traced: reproducing shallow reasoning, reinforcing its patterns, and accelerating its spread4. But amplification is not an inevitability. While LLMs inherit the ambiguity and rhetorical shortcuts of human language, their behavior is shaped as much by architecture as by data. Prompt structures, interaction design, system incentives, and delivery context all influence what kind of reasoning they surface or suppress. In the wrong configuration, they amplify fallacies. But in the right one, they could help users notice what‚Äôs missing, slow down when it matters, and build with more structure. The challenge isn‚Äôt to perfect reasoning, but to scaffold it‚Äîto make its presence more visible, its absence harder to miss, and its structure easier to sustain. FallacyTag is one such attempt‚Äîbut it‚Äôs not offered as proof of concept for the broader thesis. If it fails to change behavior or improve discourse, that outcome wouldn‚Äôt undermine the core claim: that reasoning can be shaped architecturally5. It might only mean this particular design‚Äîor this moment‚Äîwasn‚Äôt the right match. What matters is the direction of inquiry: building systems that make reasoning easier to see, to question, and to improve. If we can design for attention, we‚Äôd better design for reasoning. Previous Next . | Deliberate reasoning draws heavily on attention, memory, and executive control. Psychologists describe it as System 2 thinking‚Äîslow and effortful, unlike our fast, intuitive defaults (Stanovich &amp; West 2000; Kahneman 2011). Neuroscience shows it activates resource-intensive brain areas like the prefrontal cortex (Baddeley 2000), while cognitive load theory explains why unfamiliar reasoning tasks quickly overwhelm us (Sweller 1988). Evolutionary accounts argue that reasoning is selectively deployed‚Äîused when the stakes justify the energy cost, especially in social argument (Mercier &amp; Sperber, 2011; Lieder &amp; Griffiths 2020). Early studies have even tied sustained reasoning to glucose depletion (Gailliot et al. 2007) or, more recently, subjective fatigue (Kurzban et al. 2013).¬†&#8617; . | Online discourse is often expressive or creative but rarely builds cumulative reasoning‚Äîa claim supported by multiple empirical studies. On Facebook, Del Vicario et al. (2016) showed that users interact primarily within ideological echo chambers, where emotionally resonant content spreads more effectively than deliberative argument. Singer et al. (2016) indicate diminished reasoning quality over time on Reddit, suggesting that platform interaction fails to reinforce sustained, cumulative reasoning. Arag√≥n et al. (2017) note that drifting threads and ignored comments result from poor structuration of argument flow. Shortall et al. (2021) observe that digital platforms often degrade discussion quality when attempting to scale deliberation and highlight the scarcity of design features that support argument accumulation, critical thinking, and structured reasoning. Collectively, these studies suggest that while digital platforms facilitate creative participation, they seldom reward or reinforce the kind of reasoning that accumulates across exchanges.¬†&#8617; . | We are aware of pockets of better reasoning on the Internet: longform forums, commentaries, Substacks, or collaborative writing spaces. The focus of this section is on what gets surfaced and rewarded at scale. The next section (2.1) explains that we are not interested in the authoritative grading of the Internet.¬†&#8617; . | Large language models may amplify fallacious reasoning if not carefully constrained. Studies have found that while LLMs can identify certain fallacies, they often misclassify, overlook, or even generate flawed arguments, particularly in nuanced or ambiguous cases. For example, Teo et al. (2025) reported high variability in LLM fallacy classification performance, especially with intent-dependent types such as intentional or equivocation. Similarly, Manickavasagam &amp; Bandara (2025) noted that even with prompting and fine-tuning, models struggle with interpretive ambiguity. External evaluations have shown that LLMs frequently generate persuasive but fallacious content in argument tasks unless guided by structured reasoning constraints (Ji et al., 2023).¬†&#8617; . | Several platforms exemplify how architectural design can scaffold cumulative reasoning online. Kialo structures debates via branching pro/con argument maps, which multiple (self-promoted but seemingly independent) educational studies and documentation show improve critical thinking, clarity, and engagement. Pol.is uses clustering algorithms to surface cross-cutting consensus, and its deployment in Taiwan‚Äôs vTaiwan platform has led to real policy outcomes (vTaiwan case study; The Computational Democracy Project case studies). LiquidFeedback supports structured proposition development, integrated deliberation and voting, and delegative democracy mechanisms used by the German Pirate Party (Kling et al., (2015)) and other civic groups (Behrens et al., 2014). These platforms illustrate that, with intentional architectural design, online reasoning can indeed accumulate meaningfully across exchanges.¬†&#8617; . | . ",
    "url": "/fallacytag/pages/02-the-architecture-of-avoidance/#table-of-contents",
    
    "relUrl": "/pages/02-the-architecture-of-avoidance/#table-of-contents"
  },"6": {
    "doc": "3. How LLMs Can Support Better Reasoning",
    "title": "3. How LLMs Can Support Better Reasoning",
    "content": " ",
    "url": "/fallacytag/pages/03-scaffolded-intelligence/",
    
    "relUrl": "/pages/03-scaffolded-intelligence/"
  },"7": {
    "doc": "3. How LLMs Can Support Better Reasoning",
    "title": "Table of Contents",
    "content": ". | 3.0 A Glimpse of What‚Äôs Coming | 3.1 What Can Work Now: A Narrower but Useful Path | 3.2 Designing Within Limits | 3.3 Nudges as Pedagogical Design | . Can large language models identify fallacies? Not reliably. However, we can set conditions that allow us to capture much of the potential value. We begin with an anecdote that showcases an LLM‚Äôs surprising moment of clarity (Section 3.0), and probe the conditions under which that clarity arises (3.1). From there, we sketch a design path that works within those limits rather than against them (3.2), and argue that the resulting interaction model may not only be sufficient, but also desirable (3.3). 3.0 A Glimpse of What‚Äôs Coming . Not long ago, I watched a well-intentioned, skillfully produced 23-minute YouTube video with my son. It explored Arrow‚Äôs Impossibility Theorem and the limitations of ranked voting systems. The analysis was methodical ‚Äî until the final minute, when the tone subtly shifted from formal analysis to civic appeal. Suddenly, ‚Äúdemocracy‚Äù was no longer a voting procedure but a cherished ideal. That Churchill quote appeared ‚Äî never a good sign1. An emotionally satisfying, logically unmoored closing followed. My antenna twitched hard enough to unseat my trucker hat. Mildly bothered but unwilling to allocate much CPU to parenting, I brushed off the moment with a few glib remarks and let GPT-4 do the explaining. I asked the reportedly shaky logician to assess the reasoning in the video‚Äôs conclusion. Without coaching or fine-tuning, the bot accurately autocompleted some fallacies that are generally thorny to detect ‚Äî such as equivocation, appeal to sentiment, and category error ‚Äî far beyond what I expected. This wasn‚Äôt rote fallacy labeling. The model demonstrated contextual sensitivity ‚Äî tracking meaning shifts, identifying rhetorical substitutions for logic, and anchoring its analysis in the argument‚Äôs actual structure. Teo et al. (2025) found that LLMs stumble on these flaws. They tested multiple LLMs on a structured fallacy classification task and found that performance was strongest for shallow, syntactically cued fallacies such as ad hominem or circular reasoning. Success rates dropped sharply when fallacies demanded contextual reasoning ‚Äî tracking shifts, ambiguity, or intent. In those cases, even large models struggled to apply definitions or reason consistently across contexts. The pattern suggests that fallacy detection isn‚Äôt just a matter of fluency, but of structural grounding ‚Äî something today‚Äôs models rarely do reliably unless the structure is already implicit in the input. And then it hit me: I had scaffolded the task. Not intentionally, I was just talking through what I wanted explained to my kid, but the result looked suspiciously like prompt engineering. Great. I‚Äôd become part of the model‚Äôs reasoning pipeline. The surprise wasn‚Äôt that the model flagged obvious errors ‚Äî but that it caught reasoning drift you‚Äôd expect it to miss. The model identified those subtler flaws accurately, not because it understood, but because I shaped the task in a way that allowed it to navigate2. A fluent moment ‚Äî but scaffolded. That‚Äôs what makes it interesting: it invites a reframing. Not ‚ÄúCan models detect fallacies?‚Äù ‚Äî but ‚ÄúWhat if we designed systems around reasoning structure, not correctness?‚Äù . That‚Äôs the hinge: fallacy detection is brittle ‚Äî brittle to tone, intent, ambiguity. But surfacing structure? That‚Äôs tractable. And if done well, transformative. 3.1 What Can Work Now: A Narrower but Useful Path . If the model needed me in the loop to think straight, at least one of us was in trouble. So I tried a simpler question: . Can a general-purpose LLM, with a plain prompt and no scaffolding, surface informal fallacies in a way that feels grounded and legible to a layperson? . Not reliably, not exhaustively ‚Äî just, sometimes, usefully. I ran a comically small experiment to test it. I went to Reddit‚Äôs r/ChangeMyView, picked the first thread that wasn‚Äôt a political minefield, and pulled three consecutive recent comments. I fed them, one by one, into GPT-4 with a basic prompt: identify any flawed reasoning and explain why. No special formatting, no chain-of-thought nudges. The thread‚Äôs claim was typical: ‚ÄúLocal politics is much more important to your life than national politics.‚Äù The replies were sincere but messy‚Äîripe for overreach. | One cited Canadian municipalities to argue national politics mattered more. GPT flagged a false analogy and unsupported empirical claim. Fair. | Another drew a neat line: local = speed bumps, national = healthcare. GPT pointed to false equivalence and oversimplification. Again, fair. | A third listed federal programs to argue that national politics is more consequential. GPT noted cherry-picking and scope shifting‚Äîraising lifetime-scale policies to counter local day-to-day effects. | . This wasn‚Äôt exactly a rigorous evaluation ‚Äî three comments, one researcher, zero inter-rater reliability. However, under plain conditions, the model provided structured and sensible feedback. This is where prior studies suggest models have the best chance of success ‚Äî and where a product like FallacyTag might be viable. 3.2 Designing Within Limits . FallacyTag isn‚Äôt a truth judge. It flags form, not verdicts. Instead of asking whether a claim is right or wrong, it highlights how the reasoning is shaped‚Äîinviting reflection rather than imposing judgment. That shift in design intent makes all the difference. Structure-surfacing is tractable because it sidesteps the hardest parts of reasoning. Here‚Äôs how that tractability shifts depending on the type of reasoning and the structure of the input: . | ¬† | Plain but Coherent Text | Disjoined Text | . | Surface-Level Fallacies | ‚úÖ Reliable | ‚ö†Ô∏è Sometimes (depends on clarity) | . | Contextual Fallacies | ‚ö†Ô∏è Possible (with scaffolding) | ‚ùå Fails: too messy / brittle | . GPT-4 can help‚Äîbut, by itself, it‚Äôs not enough of an answer. The earlier examples worked because the task was shaped to fit the model‚Äôs strengths: clear text, stable discourse, no ambiguity about roles. The model followed a structure I didn‚Äôt even realize I was providing. That‚Äôs not product behavior‚Äîit‚Äôs scaffolded response. Why doesn‚Äôt that generalize? Because the hard parts remain hard. LLMs still flounder with messy discourse, shifting tone, unstable goals. They don‚Äôt know when they‚Äôre helping‚Äîespecially when reasoning becomes social, not just logical. A correct tag, surfaced at the wrong moment, can derail reflection. Reasoning isn‚Äôt just about logic. It‚Äôs about timing, tone, and trust. That‚Äôs not about accuracy. It‚Äôs about fit‚Äîthe difference between a system that clarifies and one that corrodes. Prompts can‚Äôt patch that gap. Prior efforts have tried: instruction tuning, custom prompting, multi-turn clarification34. These work in ideal settings, with motivated users.567 They don‚Äôt scale to noisy, asynchronous discourse. FallacyTag takes a different tack: it accepts the limit and designs within it. No general truth judgments. No deep inferences. Just structural cues surfaced where the model‚Äôs fluency is stable‚Äîand offered only when useful. So the MVP has to respect both constraints: . | Model limits define what can be flagged. | Interface design defines how‚Äîand whether‚Äîit should be shown. | . That path is narrow but viable: sentence-level cues, tuned to form over content, optional and deferential. FallacyTag doesn‚Äôt try to be right‚Äîit tries to help. It surfaces the shape of reasoning: binary frames, abrupt pivots, unstable structure. Sometimes that shape hints at a fallacy; sometimes it just invites a second look. This isn‚Äôt correctness automation. It‚Äôs structural affordance. And that‚Äôs the bet: that even a soft, nonjudgmental tag ‚Äî if offered at the right time, with the right tone ‚Äî can open space for reflection. 3.3 Nudges as Pedagogical Design . Leo Tolstoy spent years trying to define art, writing a dense philosophical tome that sprawls across almost 200 pages‚Äîonly to arrive at a surprisingly simple idea: art is the transmission of emotion from artist to audience. He could have said it in a few paragraphs. After reading this section, you may be able to relate to Tolstoy‚Äôs readers. The interaction model outlined in Section 3.2‚Äîwhere the system highlights reasoning that may warrant another look without outright calling it fallacious‚Äîis not just a fallback when the model is uncertain. It may be the best way to promote critical thinking, precisely because of how it distributes both cognitive and social responsibility. What we discovered on our trek through the Himalayas, driven by technical necessity, was always nearby, on level ground, with signs, if we looked through the lens of pedagogical virtue. From a learning science perspective, brief cognitive interruptions that demand user engagement are often more effective than immediate explanations. Theories of desirable difficulties8 show that introducing effortful, nontrivial steps‚Äîsuch as prompting a learner to retrieve or self-explain‚Äîcan improve both retention and transfer. Rather than simply delivering the ‚Äúcorrect‚Äù fallacy label, a nudge invites the user to reflect: Could something be wrong here? Why might this feel off? This preserves the productive discomfort that is central to metacognitive learning9, aligning with research that shows that prompting reflection improves the quality of argumentative writing more than providing corrective feedback alone10. Socially, nudges avoid the defensive posture often triggered by direct correction. In domains where argument intersects with identity‚Äîsuch as politics, culture, or values‚Äîovert labeling of fallacies can be perceived as dismissive or adversarial. Behavioral research on reactance11 suggests that people are more likely to resist information when it threatens autonomy. Soft interventions such as prompts or open-ended cues tend to preserve engagement, especially in mixed-audience or ideologically diverse environments. A system that says, ‚ÄúYou may want to rethink this‚Äù invites participation; one that says, ‚ÄúYou‚Äôre wrong‚Äù invites rejection. Figure: The surprising power of a soft touch. FallacyTag‚Äôs ‚Äúnudge‚Äù design isn‚Äôt a fallback. It‚Äôs a convergence point: four fields of research point to the same principle‚Äîpeople think better when they‚Äôre invited to reflect, not told they‚Äôre wrong. This approach also scales better across uncertain or ambiguous cases. In many instances, what appears to be a fallacy may hinge on context, unstated assumptions, or interpretive nuance. Flagging such moments as ‚Äúpotentially problematic‚Äù rather than conclusively fallacious reflects appropriate epistemic humility. It helps the system avoid overclaiming and keeps the user informed, encouraging co-interpretation rather than authority-based override. Finally, this model has the potential to shift norms12. A system that reliably surfaces ‚Äúmoments worth pausing over‚Äù could, at scale, foster new habits of attentiveness. Rather than modeling omniscient critique, it models curiosity. And rather than framing fallacy detection as adversarial or corrective, it reframes it as an act of intellectual care: paying attention to structure not to win, but to gain a deeper understanding. In this way, what began as a workaround for model fallibility becomes a design stance toward better reasoning‚Äîboth human and machine. Designing for fallacy detection under uncertainty might seem like a compromise. But as the shape of the solution emerges‚Äîfrom selective signaling to user-side engagement‚Äîit begins to look less like a concession and more like a blueprint for something better: a reasoning aid that doesn‚Äôt dictate truth, but invites rigor. One that foregrounds structure without overreach‚Äîa useful pattern not just for machines, but for people. Next, we‚Äôll explore what it means to build from those boundaries. Previous Next . | Antifreeze &amp; Coolant is the worst shampoo ‚Äî except for all the others in my garage. No, we shouldn‚Äôt submit to Churchill‚Äôs appeal to relative privation and stop seeking better forms of government.¬†&#8617; . | The source material helped: a serious, methodical, good-faith analysis with a single jarring tonal shift. In benchmark terms, this was a clean input ‚Äî not a chaotic social media thread. The conditions were ideal for the model‚Äôs strengths.¬†&#8617; . | Du, K., Xing, F., Mao, R., &amp; Cambria, E. (2024). The authors demonstrate that GPT-3.5 and PaLM-2 lack robust reasoning ability even in a structured domain like finance, particularly in tasks involving comparative and numerical reasoning. Performance improved significantly only when tasks were scaffolded with attribute-specific prompts‚Äîsuggesting that without external structure, reasoning degrades even under domain constraints.¬†&#8617; . | Lim, G., &amp; Perrault, S. (2024). While GPT-4 achieved high accuracy on a curated fallacy classification task, success required prompt tuning and failed to generalize to subtler or interpretive fallacies. The authors caution against over-interpreting such results as evidence of deep reasoning capacity, especially for real-world applications.¬†&#8617; . | Manickavasagam, A., &amp; Bandara, D. (2025). Logical fallacy detection performance improves with zero-shot chain-of-thought prompting and task-specific fine-tuning; however, success is highly sensitive to the prompt format and context. Even with a tuned GPT-3.5 model, accuracy varied across fallacy types, with interpretive and intent-based categories remaining brittle.¬†&#8617; . | Alhindi, T., Chakrabarty, T., Musi, E., &amp; Muresan, S. (2023). Their method uses multitask prompts and dataset-aware formatting to improve generalization across structured fallacy corpora. However, success still depends on having clean data and ideal conditions‚Äîfallacies are labeled ahead of time, prompts are structured, and ambiguity is minimized.¬†&#8617; . | Schneider, J., Haag, S., &amp; Kruse, L. (2025). In a negotiation game with GPT-3.5, the authors demonstrate that users can ‚Äúhack‚Äù the model through strategic misdirection, repetition, or invented flaws‚Äîhighlighting not only reasoning vulnerabilities but also the inaccessibility of prompt engineering to most participants. The wide variance in outcomes exposes serious gaps in AI literacy and reliability.¬†&#8617; . | Bjork, R. A., &amp; Bjork, E. L. (2011). Making things hard on yourself, but in a good way: Creating desirable difficulties to enhance learning. In D. S. Dunn (Ed.), Psychology and the Real World: Essays Illustrating Fundamental Contributions to Society (pp. 56‚Äì64). Worth Publishers.¬†&#8617; . | Schraw, G., &amp; Dennison, R. S. (1994). This paper introduces a widely used framework for assessing metacognitive regulation. The distinction between cognition and metacognition is helpful here: FallacyTag doesn‚Äôt merely flag content errors‚Äîit prompts awareness of reasoning itself, nudging users toward reflection about how they think, not just what they say.¬†&#8617; . | Nussbaum, E. M., &amp; Kardash, C. M. (2005). In this study, students who were prompted to reflect or generate counterarguments produced more sophisticated arguments than those who received direct corrective feedback. It supports the idea that gentle prompts to reconsider structure may be more educationally valuable than simply identifying a fallacy label.¬†&#8617; . | Brehm, S. S., &amp; Brehm, J. W. (1981). Psychological Reactance: A Theory of Freedom and Control. Academic Press.¬†&#8617; . | See 2. The Architecture of Avoidance, where we traced a positive feedback loop that erodes public reasoning by making structure harder to see and therefore less likely to be used. If FallacyTag can surface structure gently, even sporadically, it might dampen that loop. That‚Äôs not just a UI choice‚Äîit‚Äôs a norm intervention.¬†&#8617; . | . ",
    "url": "/fallacytag/pages/03-scaffolded-intelligence/#table-of-contents",
    
    "relUrl": "/pages/03-scaffolded-intelligence/#table-of-contents"
  },"8": {
    "doc": "4. Definitin and Demo",
    "title": "4. Definitin and Demo",
    "content": " ",
    "url": "/fallacytag/pages/04-introducing-fallacytag/",
    
    "relUrl": "/pages/04-introducing-fallacytag/"
  },"9": {
    "doc": "4. Definitin and Demo",
    "title": "Table of Contents",
    "content": ". | 4.1 Demo: What You‚Äôre Seeing | 4.2 Tagging Loop | 4.3 Related Work | . FallacyTag is a lightweight reasoning aid‚Äîa subtle seismograph for argument structure. It doesn‚Äôt declare truth or call out errors. It just picks up on small tremors in reasoning‚Äîmoments where things might not follow‚Äîand sends a gentle signal your way. The nudge is quiet, contextual, and optional. You‚Äôre free to ignore it. Or to pause, look closer, and decide for yourself. 4.1 Demo: What You‚Äôre Seeing . Your browser does not support the video tag. üëâ Try it yourself: coherentdrift.github.io/fallacytag/demo . The demo reveals more than an interface. The tagged text isn‚Äôt synthetic‚Äîit was generated using the prompt strategy implied in the previous section, applied to a real published opinion article1. What you‚Äôre seeing is not just a surface mockup, but an early sketch of what FallacyTag can become: a reasoning-aware layer that pairs content, critique, and context. 4.2 Tagging Loop . The core of FallacyTag can be traced in a single arc: . | A piece of content is analyzed | A reasoning flaw is detected | A tag is gently surfaced | . Then the loop pauses‚Äîfor the user. This process is visualized below: . Figure: The FallacyTag interaction loop. Tags are rendered on reasoning flaws, and users may dismiss, revise, or ignore them‚Äîeach action feeding back differently into the system. The system doesn‚Äôt push. It marks a moment of uncertainty‚Äîand steps back. From there, it depends on whether you‚Äôre reading or writing‚Äîand the kind of attention you‚Äôre bringing to the page. Some readers will nod, mentally flag the moment, and move on. Others may tap the tag to expand it‚Äîor mark it as incorrect. In some interfaces, dismissing a tag may even remove the highlight‚Äîquietly affirming the user‚Äôs decision and restoring flow. And some‚Äîif they‚Äôre in writing mode‚Äîmay return to the source and revise it. Each path keeps agency in the user‚Äôs hands. The tag never insists. FallacyTag allows users to mark a tag as helpful or incorrect, but whether that feedback can meaningfully improve the system is an open question. 4.3 Related Work . We don‚Äôt know how to build better models, but we hope to make better readers. FallacyTag draws on recent research on how large language models (LLMs) can detect logical fallacies in natural language. Recent studies have fine-tuned models like GPT-3.5 and evaluated open-source LLMs (e.g., LLaMA, Phi, Gemma) on benchmark datasets, with techniques like zero-shot chain-of-thought prompting significantly boosting classification performance2. However, the academic work generally treats fallacy detection as a self-contained NLP task, focusing on accurately classifying errors. FallacyTag has a different approach. It considers model outputs not as definitive labels but as interpretive cues‚Äîpresented in context, embedded in text, and always optional. The goal is not to perfect detection3 but to promote perception: to help users recognize when reasoning may not hold. This reframing places FallacyTag closer to a pragmatic tradition of argument analysis. Rather than policing correctness, it supports reflection and dialogue‚Äîechoing theories that view fallacies as context-sensitive missteps rather than fixed logical violations4. FallacyTag also works in messier terrain. While many benchmarks focus on short, synthetic examples, FallacyTag engages with real-world discourse, including essays, transcripts, and commentary. Its purpose isn‚Äôt to declare ‚Äúwrong.‚Äù It‚Äôs to gently ask: ‚ÄúMight this not follow?‚Äù . FallacyTag operates on transcripts‚Äîtextual representations of speech, video, or writing‚Äîenabling it to work across various formats. But the way it lands depends on the medium. That‚Äôs the challenge we take up next. Previous Next . | The full prompt is available inside the demo.¬†&#8617; . | See Teo et al. (2025) and Manickavasagam &amp; Bandara (2025) for overviews of LLM-based fallacy detection. These studies fine-tuned models like GPT-3.5 on datasets such as LOGIC and LOGIC Climate, achieving F1 scores above 0.8 in some configurations using chain-of-thought prompting and few-shot learning.¬†&#8617; . | To be clear: we are not objecting to others perfecting detection - it‚Äôs a valuable goal, and it‚Äôs clear why researchers focus on it. We do not contribute to that effort in this paper.¬†&#8617; . | This perspective is drawn from the work of Douglas Walton and Frans van Eemeren, who argue that fallacies are better understood as context-dependent breakdowns in dialogue rather than as fixed, formal violations of reasoning. See also historical discussions in Hamblin (1970) as well as later developments in computational argumentation theory (e.g., argumentation frameworks, dialogue games, and defeasible reasoning systems treat fallacies as strategic moves gone wrong or contextually inappropriate actions.)¬†&#8617; . | . ",
    "url": "/fallacytag/pages/04-introducing-fallacytag/#table-of-contents",
    
    "relUrl": "/pages/04-introducing-fallacytag/#table-of-contents"
  },"10": {
    "doc": "5. Mediums, Modalities and Fit  ",
    "title": "5. Mediums, Modalities and Fit",
    "content": "Designing FallacyTag for Text, Audio, and Video . ",
    "url": "/fallacytag/pages/05-modalities-and-fit/#5-mediums-modalities-and-fit",
    
    "relUrl": "/pages/05-modalities-and-fit/#5-mediums-modalities-and-fit"
  },"11": {
    "doc": "5. Mediums, Modalities and Fit  ",
    "title": "Table of Contents",
    "content": ". | 5. Mediums, Modalities and Fit . | 5.1 Text: Where Reasoning Is Most Visible . | 5.1.1 Adapter Implications (Technical Layer) | 5.1.2 Observed Modalities (Interaction Layer) | 5.1.3 Fit and Design Implications | . | 5.2 Audio: Designed for Flow, Not Friction . | 5.2.1 Adapter Implications (Technical Layer) | 5.2.2 Observed Modalities (Interaction Layer) | 5.2.3 Fit and Design Implications | . | 5.3 Video: The Tightrope Between Pacing and Pause . | 5.3.1 Adapter Implications (Technical Layer) | 5.3.2 Observed Modalities (Interaction Layer) | . | 5.3.3 Fit and Design Implications | 5.4 Trade-Offs Across the Flow‚ÄìFriction Spectrum | 5.5 From Logic to Delivery: Adapting the Pipeline | . | . FallacyTag is built around language. But language unfolds differently depending on where‚Äîand how‚Äîwe encounter it. A Reddit debate, a scripted podcast, and a live Twitch stream all rely on words, but differ radically in pace, tone, and user intent. These differences aren‚Äôt cosmetic. They shape how reasoning is presented, perceived, and whether it can be usefully tagged. Two dimensions matter. First, the medium‚Äîtext, audio, video‚Äîshapes the technical surface where tagging can occur. Adapters are medium-specific: a browser DOM, a podcast transcript, a video caption stream. This is the entry point for implementation. But equally important is the modality: the interaction pattern through which language unfolds. A Reddit thread, a podcast monologue, and a live Twitch stream are all built on language, but they differ in tone, timing, pace, and user expectation. These elements govern whether the reasoning structure is present, visible, and taggable. In short, medium shapes delivery; modality shapes fit. Figure: Taxonomy of reasoning contexts by medium and modality . We walk through each medium‚Äînot because it‚Äôs the dominant factor, but because each hosts a characteristic range of modalities with recurring challenges and affordances. We close with a synthesis of how these patterns inform adapter design and feedback orchestration. This discussion also fills a gap in prior work. While most fallacy detection research emphasizes classification accuracy and model performance, relatively little attention has been paid to how reasoning feedback should be delivered across modalities. To our knowledge, no existing system addresses this challenge directly. We aim to surface it here. We walk through each medium‚Äînot because it‚Äôs the dominant factor, but because each hosts a characteristic range of modalities with recurring challenges and affordances. We close with a synthesis of how these patterns inform adapter design and feedback orchestration. This discussion also fills a gap in prior work. While most fallacy detection research emphasizes classification accuracy and model performance, relatively little attention has been paid to how reasoning feedback should be delivered across modalities. To our knowledge, no existing system addresses this challenge directly. We aim to surface it here. 5.1 Text: Where Reasoning Is Most Visible . 5.1.1 Adapter Implications (Technical Layer) . Text is where FallacyTag is most natural to implement. The medium exposes a well-structured surface‚ÄîDOMs, Markdown documents, transcripts‚Äîonto which tags can be injected with minimal friction. Parsing is straightforward. Delivery can be inline or layered, synchronous or deferred. In many cases, existing user-facing interfaces (e.g., browsers, e-readers, collaborative editors) can be augmented via extensions or overlays, without requiring platform integration. This makes text a clear first target for adapters. 5.1.2 Observed Modalities (Interaction Layer) . But ‚Äútext‚Äù encompasses a wide range of modalities, shaped by platform dynamics, pacing, and user goals. Examples include: . | Deliberative (Reddit structured debate threads, peer feedback on learning Platforms like Khan Academy or Coursera, where learners receive logic-aware reviews) | Persuasive (Substack or Medium blog posts, which mix persuasion and commentary, often with explicit argument structure) | Informal (comment threads, general web reading - from essays to corporate blogs) | . Despite variability in tone, many of these environments expose reasoning in ways that make constructive surfacing possible. 5.1.3 Fit and Design Implications . FallacyTag works best when reasoning is visible, valued, and revisable. This includes: . | Longform argumentation (e.g., newsletters, essays) | Structured feedback and review (e.g., peer critique systems) | Reflective educational contexts (e.g., student forums or tutoring platforms) | . In these cases, tagging can appear inline with minimal disruption. Users can hover, tap, or click for explanations, and respond with lightweight feedback (‚Äúthanks,‚Äù ‚Äúincorrect,‚Äù etc.) These modalities support both reader-facing workflows (surfacing reasoning in content consumption) and author-facing workflows (offering logic-aware cues during drafting or revision). This dual role‚Äîsupporting both readers and writers‚Äîmakes text-based environments an especially compelling fit for FallacyTag. In some structured environments‚Äîparticularly in education‚ÄîFallacyTag may even support manual tagging or confirmation by educators, reviewers, or peer authors. These hybrid human+AI workflows are not the primary design path, but they open up new roles for reasoning-aware authoring support. By contrast, modalities like adversarial comment threads or outrage-driven forums are low-fit environments. Even well-calibrated tags risk being ignored or reinterpreted as aggression, irony, or partisanship. In such spaces, tags should be optional, passive, or private by default, and feedback mechanisms should account for tone volatility. 5.2 Audio: Designed for Flow, Not Friction . 5.2.1 Adapter Implications (Technical Layer) . Audio presents a more constrained surface for tagging. There is no native text layer; tags must anchor to transcripts, timestamps, or linked metadata. Adapters must handle: . | Transcript alignment: ensuring tags map to precise audio segments | Trigger design: determining how tags are accessed‚Äîe.g., via playback-linked cues, transcript overlays, or post-hoc bundles | Playback integration: embedding tags without disrupting listening flow | . Some platforms (e.g., Overcast, Spotify) support enriched metadata or interactive transcripts; others do not. Delivery remains contingent on third-party APIs or browser-based augmentation. The core UX challenge is this: FallacyTag is designed as a nudge‚Äîa lightweight prompt to pause and think. But most audio platforms are built around flow. Listeners may be jogging, driving, or multitasking. Their goal is sustained engagement, not interruption. What is supposed to happen when a tag appears? Should the listener stop? Remember the moment? Open their phone? These conditions make not just technical delivery harder, but reflection itself less viable‚Äîespecially in the moment. 5.2.2 Observed Modalities (Interaction Layer) . | Audio modalities tend to fall into three categories: | Scripted monologues: podcasts, essays read aloud, and commentary | Conversational formats: interviews, live chats, or recorded panels | Ambient streams: background listening where attention is partial | . Spoken formats carry arguments wrapped in rhythm, tone, and persona‚Äîoften more performance than premise-evidence-conclusion. Listeners ‚Äúfeel‚Äù the arc without always registering its logical scaffolding. Unlike text, you can‚Äôt scan back to study how pieces fit. Fallacies are harder to notice in speech because the format mutes structure. Still, scripted podcasts‚Äîespecially those with educational, analytic, or editorial aims‚Äîoffer an opening. Transcripts exist. Reasoning is present. User interest is aligned. 5.2.3 Fit and Design Implications . FallacyTag is most viable in scripted or semi-structured audio, where reasoning is clear and transcripts are available. In these contexts, tags can be surfaced via interactive transcripts, playback-linked cues (e.g., ‚Äúüß† at 18:32‚Äù), or bundled into post-listening summaries. Each route depends on platform capabilities, but all share the constraint of limited user attention. Real-time interaction remains difficult. Audio is often consumed while multitasking, and feedback mechanisms like taps or swipes are rarely accessible in the moment. Even simple reactions (‚Äúhelpful?‚Äù) may be ignored. In most cases, tagging must either be subtle and asynchronous or deferred entirely. In audio, the tradeoff between flow and friction becomes especially sharp. Here, the design challenge is to support reflection without disrupting flow. FallacyTag must act more like a whisper than a poke‚Äîa reminder stored for later, not a demand made now. On some platforms, transcript overlays may be sufficient. Elsewhere‚Äîespecially in live or mobile-first audio‚ÄîFallacyTag may need to shift from live nudges to post-listening review tools‚Äîa better fit for learners, researchers, or other serious listeners. 5.3 Video: The Tightrope Between Pacing and Pause . 5.3.1 Adapter Implications (Technical Layer) . Video combines the challenges of audio with the advantages of screen-based interaction. While users are watching‚Äînot reading‚Äîmany platforms provide caption streams or machine-generated transcripts, giving FallacyTag a viable text layer for analysis. Adapters can anchor tags to timestamps or transcript segments and surface them via: . | On-screen overlays (e.g., floated annotations synced to playback) | Transcript enhancements (e.g., scroll-sync with tagged spans) | Post-video summaries (e.g., logic digests with jump links) | . Compared to audio-only formats, delivery is more tractable: the user is already in front of a screen, making taps, hovers, or pauses more reasonable. Still, the interaction surface is shared with dynamic visuals and often attention-grabbing content, making subtlety essential. 5.3.2 Observed Modalities (Interaction Layer) . Video modalities vary widely in tone, purpose, and pacing. Strong candidates include: . | Educational explainers (e.g., Veritasium, Kurzgesagt), where structured reasoning is scripted and visualized | Instructional platforms (e.g., Khan Academy, edX, Skillshare), where transcripts align tightly with lesson logic | Live or recorded meetings (e.g., Zoom with auto-captions), where tagging might aid review or live moderation | . Narrated product demos or pitches, mainly when persuasive framing drives the narrative . These modalities range from tightly produced to semi-improvised, but many share two useful traits: visible reasoning structure and a receptive audience mindset. 5.3.3 Fit and Design Implications . FallacyTag is best suited for¬†scripted or semi-scripted videos, especially when transcripts or caption streams are aligned with the¬†argument structure. Delivery options include: . | Timed overlays, which surface tags as the speaker reaches key lines | Enhanced transcript panes, with reasoning highlights linked to playback | Post-video summaries, offering logic maps or flagged claims with timestamps | . Real-time feedback remains limited; users are watching, not interacting. But light-touch affordances‚Äîe.g., dismissible overlays or hover-to-expand prompts‚Äîcan enable interaction without disrupting flow. Depending on platform constraints, tags may be precomputed and anchored or generated dynamically from caption streams. Like audio, video demands flow‚Äîbut unlike audio, it competes for attention with movement, cuts, and visual effects. The challenge is not just technical, but also cognitive: to surface logic cues in ways that feel timely yet non-intrusive, precise but light‚Äîthat is, aligned with moments of cognitive slack. Audio resists interruption. Text invites it. Video sits between them, demanding careful threading of signal and subtlety. Figure: Flow vs. Friction Spectrum . 5.4 Trade-Offs Across the Flow‚ÄìFriction Spectrum . Each of these modalities presents different trade-offs, but the goal remains the same: to surface flawed logic gently. Some environments make that easier than others. Here‚Äôs how they compare: . | Modality | Fit | User Effort | Rationale | . | Text | Best | Low | Users are already in a reading context; inline tags feel natural; arguments tend to be structured | . | Video | Good | Medium | Screen-focused consumption makes pause-and-read viable; many videos are tightly scripted | . | Audio | Weak | High | Users are engaged in flow activities; real-time logic nudges may break attentional rhythm | . That‚Äôs why our initial focus is on browser-based reading, structured peer feedback, and lightly augmented video playback. These modalities offer the best trade-off between cognitive fit and delivery feasibility. Users are already primed to engage with reasoning, and the technical surfaces support timely, low-friction interventions. In cases where multiple representations exist (e.g., video + transcript), FallacyTag aims to provide consistent tagging across views, preserving reasoning cues even as interaction modes shift. The delivery format may influence the style or granularity of the explanation, but the underlying structure remains stable. These starting points also offer a scalable base for broader deployment. Browser overlays and transcript-based adapters require no deep platform integration and can reach users across many sites and tools. By starting where both infrastructure and attention are available, FallacyTag can refine its methods before tackling harder environments, such as real-time speech, mobile-first platforms, or visually dense media. 5.5 From Logic to Delivery: Adapting the Pipeline . FallacyTag uses the same core reasoning engine across all media, but the execution path varies by medium. Input format, UI surface, feedback channel, and timing model all shift depending on whether the content is read, heard, or watched. These differences aren‚Äôt just cosmetic‚Äîthey affect what‚Äôs possible and what makes sense. The table below breaks this down. Text-based deployments allow for inline tagging with low friction and rich user feedback. Audio, by contrast, requires transcript anchoring and post hoc interaction. Video sits in the middle: it benefits from visual anchors and screen presence, but still contends with timing sensitivity and competing visual demands. | Aspect | Text-Based Reading | Audio-Only Listening | Audio-Visual (Video) | . | Input Format | Raw or DOM-parsed text (Reddit, Medium) | Auto-generated transcript (podcast, talk) | Captions/transcript (YouTube, Zoom) | . | Preprocessing | Inline span segmentation, quote filters | Timestamped text extraction, speaker cues | Caption alignment, sentence stitching | . | UI Integration | Inline overlays, hover or click to expand | Transcript summary, timestamped tag index | Scroll-sync overlays, caption-layer tags | . | Timing Model | Real-time or on-load tagging | Post hoc only (non-interruptive) | Inline or scroll-synced, possibly deferred | . | Feedback | Direct interaction (dismiss, expand) | Optional post-listening rating or flag | Tap/click during playback or review mode | . | UX Constraints | Must not disrupt reading flow | Tags must avoid intrusion during listening | Must avoid screen clutter, timing friction | . The architecture diagram sketches how these medium-specific layers fit into the system as a whole: . Figure: How FallacyTag Adapts to Text, Audio, and Video Surfaces . While the core architecture treats all media‚Äîtext, audio, and video‚Äîas inputs to a shared reasoning pipeline, the implementation diverges at the delivery layer. A Reddit comment and a YouTube transcript may flow through the same core inference engine, but the way tags are rendered, timed, and interacted with varies significantly. Browser-based overlays may appear inline during reading; podcast transcript tags might index timestamped summaries post hoc; video overlays might sync with caption lines during playback. These differences don‚Äôt change the core logic, but they do shape user experience, latency expectations, and interface behavior. Delivery orchestration is thus surface-specific, even when the underlying reasoning logic stays consistent. The divergence isn‚Äôt just architectural‚Äîit‚Äôs experiential. Each modality shapes how the user perceives the tag, when they can respond, and what kind of feedback is even possible. To visualize how this plays out in practice, the following figure walks through representative system flows for each medium. All begin with input parsing and reasoning detection. But from there, the user experience diverges: . | Text tags appear inline, with full interaction and feedback | Audio tags link to moments in time, with optional voice/button triggers | Video tags float visually, balancing timing, clarity, and minimal interruption | . Figure: How the tagging pipeline plays out differently for text, audio, and video. Though the reasoning logic is shared, the form of nudging is not. This is the core tension FallacyTag is built to navigate: logic wants to be surfaced, but attention doesn‚Äôt want to be broken. The tradeoff‚Äîflow versus friction‚Äîvaries by medium. What works for a blog post may misfire in a podcast. FallacyTag‚Äôs logic layer stays constant, but its delivery adapts‚Äîalways tuned to where users are and how they engage. FallacyTag‚Äôs delivery must adapt to each surface, but behind that flexibility lies a common core. Whether analyzing a Reddit post, a podcast transcript, or a captioned video, the same system interprets the text, identifies potential reasoning flaws, and returns structured tags. The variability lies in how those tags are surfaced and received. In the next section, we pull back the curtain on how this shared pipeline works, step by step. Previous Next . ",
    "url": "/fallacytag/pages/05-modalities-and-fit/#table-of-contents",
    
    "relUrl": "/pages/05-modalities-and-fit/#table-of-contents"
  },"12": {
    "doc": "5. Mediums, Modalities and Fit  ",
    "title": "5. Mediums, Modalities and Fit  ",
    "content": " ",
    "url": "/fallacytag/pages/05-modalities-and-fit/",
    
    "relUrl": "/pages/05-modalities-and-fit/"
  },"13": {
    "doc": "6. How It Works",
    "title": "6. How It Works",
    "content": " ",
    "url": "/fallacytag/pages/06-how-it-works/",
    
    "relUrl": "/pages/06-how-it-works/"
  },"14": {
    "doc": "6. How It Works",
    "title": "Table of Contents",
    "content": ". | 6.0 System Architecture: Core Pipeline . | üîÅ System Flow (Conceptual Sketch) | . | 6.1 Key Components . | üìÖ Input Source | ü™ú Preprocessing &amp; Parsing | üß† Inference Engine (LLM) | üè∑Ô∏è Tag Generation | üñºÔ∏è UI Overlay &amp; Display | üëç User Feedback Layer | üìä Telemetry Aggregator (Opt-In) | üõ†Ô∏è Heuristic Tuning / Prompt Updates | . | 6.2 Implementation Spectrums and Design Posture . | Variation Point 1: Privacy ‚Üî Observability | Variation Point 2: Interpretability ‚Üî Expressive Power | Variation Point 3: Statelessness ‚Üî Adaptivity | Variation Point 4: Reflection ‚Üî Enforcement Drift | Variation Point 5: Separation ‚Üî Intrusion | . | 6.3 One Configuration: Functional and Non-Functional Design Commitments . | ‚úÖ Functinal Alignment | üîí Configuration Commitments (How This Variant Behaves) | . | 6.4 Architectural Risks and Limitations | . 6.0 System Architecture: Core Pipeline . FallacyTag is a modular system designed to surface informal reasoning flaws in near real time‚Äîsupporting reflection, not enforcement. Its architecture scaffolds awareness without overstepping into correctness policing or ideological filtering. Every design decision‚Äîfrom sentence-scoped inference to symbolic tagging and optional feedback‚Äîserves this posture: to promote interpretability, protect agency, and enable insight without control. While the core engine is shared across media, execution paths diverge by modality‚Äîparticularly in how fallacies are surfaced, interpreted, and acted upon. As detailed in Section 5.5, text allows inline tagging and real-time interaction; video supports scroll-synced overlays or delayed review; and audio restricts feedback to post-hoc summaries or ratings. These constraints don‚Äôt just shape the how of delivery‚Äîthey influence what can be surfaced at all. Subtle or interpretive fallacies may be reserved for modalities with richer context and more forgiving UX, while simpler, structurally distinctive fallacies can be safely tagged across formats. This section sets aside those delivery-layer specifics to focus on the core, modality-agnostic pipeline that drives all deployments. üîÅ System Flow (Conceptual Sketch) . The conceptual pipeline below illustrates the architecture‚Äôs top-down flow: . Figure: System Flow Overview . The pipeline flows top-down, from raw input to user-visible tags and optional telemetry. Each stage‚Äîpreprocessing, inference, tagging, display, feedback‚Äîis modular, enabling platform flexibility, targeted testing, independent upgrades, and runtime introspection across deployment environments. Inference execution may vary by context: synchronous for small browser-based use, and batched or asynchronously queued in larger-scale or server-hosted deployments. Future deployments may include policy-based model routing, dynamically selecting among local, remote, or compressed inference backends based on platform constraints, latency budgets, or privacy posture. Failures in one component (e.g., inference timeout or malformed tag) are quarantined by design, allowing downstream stages to continue without disruption. 6.1 Key Components . Each pipeline component plays a distinct role in transforming input into interpretable tags. Though modular, they work in concert to uphold FallacyTag‚Äôs core commitments: scoped reasoning, symbolic clarity, and user-centered feedback. Below, we walk through each pipeline component shown above, clarifying its role, boundaries, and behavior. üìÖ Input Source . The system operates on textual transcripts, regardless of the original input medium. For native text sources, such as Substack posts or article excerpts, the transcript is extracted and segmented. For audio and video sources, transcripts are obtained either from host platforms (e.g., YouTube captions) or generated via automatic speech recognition (ASR) when no aligned transcript is available1. Once in transcript form, the input is segmented into interpretable units. For text, this typically means sentence- or paragraph-level chunks. For audio and video transcripts, the system preserves timestamp alignments to support downstream tagging and review. Segmentation aims to balance contextual coherence with UI anchorability: longer spans improve reasoning fidelity, but may be harder to surface precisely in the interface. This layer may also apply domain-specific filters‚Äîe.g., limiting analysis to opinion sections or speaker turns longer than 10 seconds. It is also responsible for privacy filtering, deduplication, and suppressing segments previously tagged or flagged. ü™ú Preprocessing &amp; Parsing . Input is cleaned, span-split, and optionally filtered (e.g., quotations, sarcasm, or unattributed fragments). This stage may incorporate platform-specific exclusions. üß† Inference Engine (LLM) . Each transcript span is evaluated by a prompt-tuned LLM, which returns . A confidence-ranked list of fallacy candidates . A reasoning span for each candidate (e.g., sentence or token range) . A short explanatory stub for the top candidate . These stubs are displayed alongside tags to support lightweight reflection, and may also inform prompt tuning or tag clustering. If the model returns invalid output or fails to meet minimal structural guarantees, the span is silently skipped, ensuring graceful degradation with no interface disruption. Inference traces‚Äîincluding prompt version, candidate list, scores, reasoning spans, and explanations‚Äîcan be logged for debugging or audit purposes, subject to deployment-specific privacy constraints. The model is not fine-tuned. üè∑Ô∏è Tag Generation . Inferred flaws are matched to a curated taxonomy of sentence-detectable, structurally distinctive fallacies (e.g., ad hominem, straw man). Each tag includes metadata for confidence, severity, and display logic. Matching is based on heuristics over LLM output patterns, confidence scores, and linguistic cues, with fallback rules for handling overlaps or ambiguity between candidate tags. Tagging behavior is versioned alongside the taxonomy, allowing detection logic to evolve without ambiguity or retroactive reinterpretation of prior outputs. The taxonomy itself is defined externally in a structured, versioned format, enabling modular updates and safe expansion as reasoning patterns or discourse norms evolve. üñºÔ∏è UI Overlay &amp; Display . Tags are rendered inline or adjacent to the original text, depending on the platform. Multiple tags per span may stack, sequence, or be summarized for clarity. The UI emphasizes minimal disruption and reader agency. Tag anchoring relies on stable identifiers‚Äîsuch as character offsets, speaker timestamps, or structural markers‚Äîdepending on the platform, to ensure resilience against layout changes or content updates. Tag display behavior‚Äîsuch as stacking thresholds, visibility toggles, or label phrasing‚Äîis governed by configurable UI policies, not embedded logic, enabling safe and targeted evolution over time. üëç User Feedback Layer . Users may agree with or dismiss tags. All feedback is local by default. Repeated actions may alter tag visibility, but never affect inference. üìä Telemetry Aggregator (Opt-In) . When enabled, telemetry aggregates anonymized feedback patterns without identity linkage, session tracking, or device fingerprinting. Tracked metrics may include tag-level dismissal or confirmation rates, frequency of no-feedback spans, and overall fallacy type distributions. These aggregate signals help surface system-level blind spots or UX friction points. Critically, this feedback is not used to retrain the model or alter inference. Its purpose is diagnostic, not adaptive: to inform future prompt versioning, guide interface tuning, and support regression testing, without compromising user privacy or autonomy. üõ†Ô∏è Heuristic Tuning / Prompt Updates . Based on population-level feedback, human reviewers may adjust prompts, thresholds, or tag phrasing. No model weights are changed. Updates are periodic, reversible, and auditable. Prompt and taxonomy changes are gated by human review and may require stability checks‚Äîsuch as regression tests against known span sets‚Äîto prevent unintended behavioral drift. Evaluation of prompt effectiveness may include aggregate dismissal patterns, manual review, and selective precision-recall estimation over curated test sets. While each component is defined here, some behaviors are intentionally left flexible. Section 6.2 explores how these components may be varied to suit different goals. 6.2 Implementation Spectrums and Design Posture . The architecture described above is not a fixed blueprint‚Äîit‚Äôs a flexible foundation. While the overall pipeline remains stable, key components can behave quite differently depending on the values and context of a particular deployment. These choices aren‚Äôt just technical‚Äîthey reflect deeper commitments. This section explores five such spectrums: recurring tensions that shape what FallacyTag is, and what it isn‚Äôt. These spectrums represent recurring, structured points of variation‚Äîplaces where thoughtful implementers are likely to diverge. In software engineering, this idea is formalized as a product line architecture (PLA): a family of related systems built from shared parts with intentional variability2. A summary of this model is available in Appendix D. Five key variation points define the FallacyTag product line. Variation Point 1: Privacy ‚Üî Observability . Options: Local-only feedback, session-based tracking, identity-linked telemetry . Constraints: Must preserve user autonomy, avoid behavioral profiling, and support aggregate system-level tuning only . FallacyTag treats user privacy as a first-order constraint. Feedback is local by default, and telemetry‚Äîwhen present‚Äîis system-level, optional, and anonymous. This posture rules out identity-linked traces or persistent profiling. However, it still allows for diagnostic aggregation: signals such as dismissal rates, tag confirmation ratios, and non-response frequencies can be collected across users in an anonymized form. This tradeoff limits the system‚Äôs ability to trace individual or session-level failures over time. Without persistent cross-user traces, it may miss finer-grained UX friction, blind spots, or regressions. That said, FallacyTag remains unconvinced that per-user feedback pipelines meaningfully improve reasoning quality. It draws a bright line between diagnostic telemetry, which supports quality assurance, and adaptive inference, which could compromise interpretability, stability, and trust. In more bounded or cooperative domains‚Äîsuch as classroom settings, research communities, or opt-in enterprise contexts‚Äîthe calculus may shift. Richer feedback collection may be acceptable, or even desired, so long as consent and transparency are preserved. In contrast, broad public deployments must prioritize minimalism to earn trust. Variation Point 2: Interpretability ‚Üî Expressive Power . Options: Sentence-level symbolic tags, discourse-aware analysis, learned tag patterns. Constraints: Must preserve transparency and attribution; hallucination and ambiguity must be minimized. The system limits analysis to sentence-level spans and matches flaws against a curated, symbolic taxonomy. This enables clean attribution and minimal hallucination. But it also excludes longer-span reasoning flaws, discourse-level inconsistencies, and many fuzzy rhetorical tactics. Some implementations may favor broader context windows or learned tag patterns. Those changes might improve coverage, but likely at the expense of transparency, stability, and user trust. For example, implementations targeting more structured or ‚Äúfriendly‚Äù domains‚Äîsuch as academic writing or formal debate‚Äîmay tolerate greater expressive complexity. These contexts often assume a higher baseline of coherence, tone, and seriousness, creating more headroom for experimental models or broader reasoning spans without destabilizing the user experience. Variation Point 3: Statelessness ‚Üî Adaptivity . Options: Prompt-only inference, session-tuned prompting (e.g., models that adapt formatting or content over short-lived sessions without persistent memory), fine-tuned models with memory. Constraints: Must support versioning and auditability; adaptive behaviors must not compromise traceability. Inference is stateless, using prompt templates rather than feedback-driven retraining. This ensures version control and resistance to drift. But it also constrains the system‚Äôs ability to adapt, whether to user preferences, emergent discourse forms, or adversarial input. Fine-tuning, memory, or session-based models may offer responsiveness, but they introduce new forms of opacity, brittleness, and alignment risk. Variation Point 4: Reflection ‚Üî Enforcement Drift . Options: Reflective tagging, optional user annotations, and downstream scoring. Constraints: Must avoid authoritative or punitive use; system intent must remain transparent . FallacyTag is built for reflection, not adjudication. It avoids verdicts. But that posture is not always preserved downstream. Tags that signal flawed reasoning may be reused‚Äîdeliberately or not‚Äîfor scoring, ranking, or moderation. Licensing and UI design can discourage misuse, but not prevent reinterpretation. Implementers should consider how their choices affect not only what the system outputs, but also how that output may be reinterpreted. Variation Point 5: Separation ‚Üî Intrusion . Options: Post-hoc Review, Inline Overlay, Export-only . Constraints: Must preserve primary experience flow; avoid disruptive or adversarial UX . Different UI integration strategies shape how users encounter tags. FallacyTag favors separation: tagging occurs post-hoc or inline in ways that are intentionally non-disruptive. But some deployments may prefer intrusive feedback‚Äîsuch as real-time overlays or active interventions‚Äîespecially in structured settings like classrooms or productivity tools. | Post-hoc review works best in slower or asynchronous workflows, such as reviewing essays, analyzing forum threads, or reflecting on meeting transcripts. | Inline overlays can be helpful for textual feedback, but risk breaking flow if overused. | Export-only modes may suit cases where tagging is detached from the original user experience. | . This spectrum shapes user trust and retention: interventions that feel overly assertive or judgmental may erode engagement, even if they improve clarity. Appendix D provides a tabular summary of these variation points, including constraints and deployment contexts. While Section 6.2 surfaces variation points across the product line, the sections that follow document one principled configuration. These decisions are neither arbitrary nor final, but they reflect an opinionated stance, optimized for clarity, trustworthiness, and feasibility. Other deployments may differ. This one is buildable. 6.3 One Configuration: Functional and Non-Functional Design Commitments . Having mapped the key axes of variability, we now describe one concrete implementation. This build draws on established reference patterns‚Äîcommon to writing tools, content moderation pipelines, and explainable AI‚Äîbut applies them with specific architectural commitments. While FallacyTag aligns with best practices in NLP and content tagging, it departs from enforcement systems and agentic LLM chains where transparency, restraint, and trustworthiness are harder to guarantee. ‚úÖ Functinal Alignment . The following functions define the baseline configuration used in current prototypes and illustrative use cases. Other deployments may adapt or extend them. | Function | FallacyTag Implementation | Proven Reference Points | Variation Point | . | Input Preprocessing | Segment into sentence-level spans, filter quotes/sarcasm | Content moderation tools (e.g., Perspective API) | Tag Strategy | . | Local Inference | Prompt-tuned LLM analyzes local structure for flaws | Symbolic NLP tagging (Turnitin, Grammarly) | Inference Type | . | Symbolic Tag Generation | Fallacy labels with confidence, stub, metadata | FactRank, argument mining tools | Tag Strategy | . | UI Overlay | In-line or adjacent tags, minimal disruption | Writing tool plugins, browser-based linters | UI Integration | . | Feedback Collection | Local-only agree/dismiss actions | Explainable AI UX patterns | Feedback Use | . | Telemetry Aggregation | Anonymized, periodic, system-level only | Apple differential privacy model, DuckDuckGo telemetry | Privacy Handling | . These patterns enable fast inference, high interpretability, and platform-agnostic deployment. üîí Configuration Commitments (How This Variant Behaves) . FallacyTag encodes architectural constraints that serve its mission: transparency, restraint, and trustworthiness. These aren‚Äôt limitations‚Äîthey are deliberate tradeoffs. 6.4 Architectural Risks and Limitations . This configuration reflects principled constraints, but those choices introduce tradeoffs. The table below surfaces key risks, grouped by system layer. These risks are specific to the current configuration. As the FallacyTag product line evolves, new variation points (e.g., adversarial robustness) may emerge, offering additional tradeoff surfaces. FallacyTag‚Äôs architecture encodes more than a pipeline‚Äîit expresses a stance. By privileging transparency over adaptation, modularity over monoliths, and reflection over adjudication, it aims to create space for clearer thinking, not to enforce it. Even if it fails to improve discourse, it shows that reasoning can be surfaced architecturally, without policing or intrusion. What‚Äôs next? . We‚Äôve shown that FallacyTag is buildable. But that‚Äôs not the same as saying it‚Äôs feasible. Section 7 steps back to ask a harder question: Could this work in the real world, not just as a system, but as a product? The analysis that follows considers technical performance, conceptual integrity, and social fit, because success depends on all three. Previous Next . | ASR-generated transcripts may include recognition errors, especially in noisy environments, with overlapping speech, or when speakers have uncommon accents. While sufficient for many use cases, these imperfections can occasionally affect tagging accuracy.¬†&#8617; . | Product Line Architecture (PLA) is a formal software engineering methodology for designing families of systems with shared components and intentional variation. It helps structure design choices across a solution space, rather than prescribing a single implementation. See Pohl et al., 2005 for an overview, or Clements &amp; Northrop‚Äôs Software Product Lines (SEI, 2001) for foundational guidance.¬†&#8617; . | . ",
    "url": "/fallacytag/pages/06-how-it-works/#table-of-contents",
    
    "relUrl": "/pages/06-how-it-works/#table-of-contents"
  },"15": {
    "doc": "7. Is It Feasible?",
    "title": "7. Is It Feasible?",
    "content": " ",
    "url": "/fallacytag/pages/07-feasibility/",
    
    "relUrl": "/pages/07-feasibility/"
  },"16": {
    "doc": "7. Is It Feasible?",
    "title": "Table of Contents",
    "content": ". | 7. Is It Feasible? . | 7.1 Technical Feasibility: Can LLMs Reliably Flag Fallacies? . | Assessment: üü¢ High (within defined boundaries) | Limitations and edge cases | Conclusion | . | 7.2 Conceptual Feasibility: Is Fallacy Detection a Coherent Task? . | Assessment: üü† Moderate‚ÄîClear potential, but epistemic ambiguity looms | What about false positives? | Conclusion | . | 7.3 Social Feasibility: Between Usefulness and Blowback . | Assessment: üü° Conditional‚ÄîUsable in selective contexts, risky in general ones | Conclusion | . | 7.4 Summary Table | 7.5: Scope Boundaries: What FallacyTag Doesn‚Äôt Do | . | . Having explored FallacyTag‚Äôs function (Sections 3‚Äì5) and structure (Section 6), we now ask: can it work in practice? This section assesses feasibility across three dimensions: technical, conceptual, and product/social, each with its own risks and mitigation paths. This isn‚Äôt a claim that FallacyTag must exist‚Äîonly that it could be useful. If it prompts reflection in even one out of five cases, that may be enough. Our conclusion isn‚Äôt a blanket yes or no. FallacyTag is feasible‚Äîbut only within a narrow operational scope, with rigorous design and bounded expectations. 7.1 Technical Feasibility: Can LLMs Reliably Flag Fallacies? . Assessment: üü¢ High (within defined boundaries) . Large language models can reliably detect certain reasoning patterns, especially in short, self-contained statements where surface cues align with known fallacy types. | Detectable forms: Ad hominem, false dilemma, and appeal to authority are among the most reliably identifiable. These patterns have stable linguistic signals and are less dependent on topic knowledge. | Example: ‚ÄúYou can‚Äôt trust her view on climate change‚Äîshe didn‚Äôt go to college.‚Äù ‚Üí Ad hominem. This structure generalizes across domains. | . | Low domain dependence: FallacyTag operates at the level of argument form, not factual accuracy or intent. It highlights patterns such as post hoc or false dilemma based on surface structure, without judging whether a claim is true or inferring a speaker‚Äôs motives. For instance, it might flag: ‚ÄúBecause the economy is strong, the government must be doing a great job‚Äù as a post hoc fallacy‚Äînot because the claim is false, but because the causal link is assumed rather than demonstrated. This structural focus keeps detection tractable and sidesteps entanglement with domain knowledge or epistemic disputes. | Example: ‚ÄúA Nobel Prize-winning economist supports this plan‚Äîso it must be correct.‚Äù ‚Üí Appeal to authority. This holds whether or not the claim aligns with truth. | . | Bounded inference minimizes error: Limiting evaluation to sentence-level spans reduces hallucination risk and avoids synthetic logic across paragraph boundaries. For a high-level system overview, see Section 6. | Precedent evidence: While no public benchmark yet exists for large-scale fallacy detection, GPT‚Äë4 demonstrates strong performance on a wide range of academic reasoning tasks. It performs well on structured, prompt-driven evaluations and is capable of recognizing certain informal fallacies in short, curated inputs. Turnitin‚Äôs Revision Assistant demonstrated that students could benefit from sentence‚Äëlevel feedback on argumentative strength, particularly when feedback was constructive, rubric-aligned, and visually scaffolded1. However, in practice, Revision Assistant remained a niche offering and was eventually phased out, with limited evidence of sustained adoption2. Similarly, IBM‚Äôs Project Debater broke new ground in claim detection, key‚Äëpoint mining, and evidence clustering, showing that large-scale argument analysis is technically feasible3. Yet in public demonstrations and broader research deployments, the system often faltered when facing rhetorical nuance, satire, or emotionally charged content, underscoring how brittle even sophisticated structures can be in open-ended discourse4. FallacyTag builds on these precedents while diverging in emphasis. Rather than scoring essays or debating humans, it offers lightweight cues for users mid-conversation. Compared to traditional rule‚Äëbased fallacy detectors, FallacyTag offers greater adaptability and linguistic flexibility, without sacrificing interpretability ‚Äîfollowing Lipton‚Äôs pragmatic emphasis on actionable clarity over transparency5. By avoiding hard‚Äëcoded triggers and relying on probabilistic surface pattern recognition, the system can respond to novel phrasings while still offering explicit reasoning stubs6. | Learning potential: A feedback layer‚Äîtracking user responses such as ‚Äòagree‚Äô, ‚Äòdismiss‚Äô, or ‚Äòunclear‚Äô‚Äîcan support interface tuning, not model retraining. These interactions might be stored locally or anonymously (e.g., via browser-local telemetry) to improve UI behavior or flag ambiguous tagging patterns. Crucially, no automatic model fine-tuning is performed based on this data. Any learning loops must be human-in-the-loop: used to inform updated heuristics or reviewed batches, not dynamic adaptation. This limits misuse (e.g., brigading) and preserves interpretability. While human-in-the-loop tuning is viable long term, no automated adaptation is planned for early-stage systems. Future experiments might explore reinforcement-style learning for selective contexts, but these remain explicitly outside the MVP scope. | . Limitations and edge cases . | Tagging is probabilistic, not deterministic. Even high-signal patterns can be falsely triggered by rhetorical language or sarcasm. For example, a satirical statement like ‚ÄúOh, sure, because the moon landing was faked, vaccines must be dangerous‚Äù might be flagged for false cause, even though the intent is ironic. Without tonal cues, models may mistake performance for assertion. | Fallacy initiation (detection) is more error-prone than follow-up explanation. | . Conclusion . A constrained MVP‚Äîfocused on a small set of structurally distinct fallacies is technically feasible with today‚Äôs tools. For early-stage deployment, the most technically tractable candidates include: . | Ad hominem | False dilemma | Appeal to authority | Hasty generalization | Post hoc (false cause) | Straw man (in limited cases where paraphrasing is clearly evident) | . These fallacies tend to have identifiable surface features and occur frequently in real-world argumentation. By contrast, more ambiguous types‚Äîsuch as slippery slope, begging the question, or equivocation‚Äîrequire context-sensitive inference and are excluded from MVP scope. 7.2 Conceptual Feasibility: Is Fallacy Detection a Coherent Task? . Assessment: üü† Moderate‚ÄîClear potential, but epistemic ambiguity looms . FallacyTag asks a delicate question: Can informal fallacies be flagged reliably, across varied contexts, without collapsing into misinterpretation or misuse? . The answer depends on how we classify fallacies‚Äînot just by name, but by their structural properties, dependence on speaker intent, and sensitivity to cultural or genre norms. Some forms are well-suited to light-touch, surface-level tagging. Others aren‚Äôt. To guide both design and deployment, we distinguish fallacies across three axes: . | Type | Examples | Implication for Tagging | . | Structural | Ad hominem, false dilemma, post hoc | Surface-level patterns with stable linguistic cues; most taggable. | . | Intent-Dependent | Straw man, red herring | Requires inferring motive or rhetorical context; often ambiguous. | . | Culturally/Genre-Variant | Appeal to tradition, appeal to emotion | Varies across discourse communities and settings; needs hedging. | . This tripartite framing aligns loosely with hierarchical models of fallacy classification, such as Lei &amp; Huang‚Äôs Logical Structure Tree (2024), which structures logical connectives and argument spans7. These systems capture important logical distinctions‚Äîbut are model-facing, not user-facing. FallacyTag‚Äôs task is different: to surface useful8 cues for humans in situ, often mid-conversation, without requiring formal argument reconstruction. While some fallacies are tightly defined, many involve judgment calls. Reasoning errors often depend on subtle distinctions between flawed structure and legitimate rhetorical variation. | Category boundaries are porous: Some fallacies exhibit overlapping structures. | Example: ‚ÄúIf we ban this one book, soon we‚Äôll have no libraries.‚Äù ‚Üí Could be slippery slope, or a provocative framing to spark debate. | . | Contextual ambiguity: The same phrase can be fallacious or not depending on tone, audience, or genre. | Example: ‚ÄúYou think the Earth is round? Galileo was jailed for saying that.‚Äù ‚Üí Appeal to authority in science writing, but possibly parody in a satirical context. | . | Philosophical depth required: Certain critiques (e.g., begging the question, circular reasoning) require close analysis of underlying assumptions, often beyond what can be inferred from surface text. | Design strategies: . | Use hedged tags (‚ÄúPossible false dilemma?‚Äù) to acknowledge interpretive space. | Provide contextual explanations that cite the local inference (not global judgments). | Keep the system‚Äôs claims minimal and non-prescriptive. | . | . What about false positives? . In FallacyTag‚Äôs design, a false positive‚Äîtagging a statement as potentially fallacious when the reasoning is sound‚Äîis treated not as a failure but as a low-cost prompt. Tags are framed as tentative cues (‚ÄúPossible false dilemma‚Äù) and can be easily dismissed. The goal is not to mark errors but to activate reflection. That said, false flags are not epistemically neutral: they can introduce noise, draw attention away from valid reasoning, or erode trust if overused. To mitigate this, FallacyTag uses conservative thresholds for tagging, prioritizes clarity over coverage, and offers localized explanations that clarify the nature of the detected structure. The system assumes that the occasional unhelpful tag is acceptable if it fosters rare but meaningful moments of insight, and if the design makes it easy to ignore what doesn‚Äôt resonate. Conclusion . As discussed in Section 7.1, FallacyTag operates on surface structure, not truth claims or speaker intent. This design choice reflects long-standing debates in informal logic: while some theorists argue that rhetorical context and motive are essential to classifying fallacies‚Äîparticularly in persuasive or adversarial speech9‚Äîothers point to a deeper tension. As early as Hamblin (1970), scholars noted the lack of a stable definition of fallacy‚Äîa challenge now known as Hamblin‚Äôs dilemma: either fallacies are too broadly defined to be analytically useful, or too narrowly defined to reflect real discourse. Subsequent frameworks, such as Walton‚Äôs argumentation schemes, attempt to formalize common failure modes but still depend on contextual interpretation. FallacyTag deliberately avoids this ontological weight. It flags structural forms that are statistically or historically associated with reasoning breakdowns, without asserting that a given instance is fallacious. These cues are meant to activate reflection, not deliver correction. The goal is not to resolve debates, but to surface patterns worth re-examining. Practically, this stance informs every aspect of the interface: tags are hedged, optional, and paired with short, local explanations. Research on metacognition supports this lightweight approach10, showing that even minimal, non-evaluative prompts can improve engagement and revision behavior. Still, even well-intended prompts risk misfire if trust, tone, or context are mishandled. Systems that aim to model reasoning rarely fail for lack of structure‚Äîthey fail for lack of scoping. FallacyTag‚Äôs constraints are not just limitations‚Äîthey are design principles meant to protect interpretive diversity. This design also learns from precedent. Turnitin‚Äôs Revision Assistant struggled to balance structure and nuance. ARG-miner and similar tools generalize poorly outside narrow research contexts. Even recent LLM-based detectors11, while capable, remain backend systems‚Äîoptimized for analytics, not user experience. FallacyTag takes a different path. It aspires to be clear without being authoritative, visible without being intrusive. If it earns trust, even in small circles, FallacyTag might help shape a culture where reasoning becomes a shared responsibility, not a private burden. What does this mean for design? FallacyTag treats fallacies not as definitive errors, but as structural signals‚Äîtentative cues to prompt reflection. By using hedged tags, localized explanations, and conservative thresholds, it stays grounded in what LLMs can reliably surface. Conceptual ambiguity isn‚Äôt a blocker‚Äîit‚Äôs a constraint to design around. 7.3 Social Feasibility: Between Usefulness and Blowback . Assessment: üü° Conditional‚ÄîUsable in selective contexts, risky in general ones . Having spent a decade living in one of those classic San Francisco buildings, where you can hear cockroaches snoring in the apartment two doors down, I passed up countless opportunities to gently point out logical fallacies in my neighbors‚Äô conversations. I am not proud of my cowardice, though I suspect it‚Äôs a minority remorse. Humor aside, the point is serious: reasoning interventions are delicate. While the technical idea is promising, social and institutional dynamics can quickly turn even neutral features into flashpoints. Consider the reaction to automated content moderation tools like YouTube‚Äôs comment filters or Facebook‚Äôs misinformation labels. Though designed to improve discourse quality, they often triggered accusations of bias, political manipulation, or speech suppression. In many cases, the mere act of labeling content‚Äîregardless of accuracy‚Äîwas seen as alignment with institutional power. Even in educational contexts, plagiarism detectors have drawn criticism for undermining trust and creating adversarial dynamics. Writing tools like GPT-based advisors and Gradescope-style rubrics have gained traction in classrooms, but not without criticism. Their success often hinges on whether they‚Äôre framed as aides to reflection or enforcement. Similarly, even systems that model reasoning structure well can fail if they overreach their context or suggest unintended authority. These examples underscore how even well-intentioned tools must navigate perception, not just function. | Risk of user misunderstanding: Some users may interpret fallacy tags as authoritative fact checks, censorship, or ideological moderation. For example, in a heated election forum, a tag like ‚Äòfalse dilemma‚Äô applied to a candidate‚Äôs supporter could be seen as delegitimizing their viewpoint, potentially leading to accusations of bias by the platform or demands for tag transparency and reversal. Such cases can snowball into PR crises or policy disputes if not anticipated and managed carefully. | Example: A comment flagged for ‚Äúappeal to emotion‚Äù in a political debate could be interpreted as bias‚Äîeven if technically justified. | . | Vulnerability to performance: Tags can be exploited rhetorically (‚ÄúYou got flagged!‚Äù) or serve as mockery. This undermines their intended purpose. | Differential trust tolerance: Structured environments like academic classes or formal debates are more accepting of logic tools. Open social platforms may view them as intrusive. | Deployment approaches: . | Opt-in overlays (e.g., browser extensions for Reddit, news sites). | Private feedback tools for writers, educators, and researchers. | Pilots in logic-focused spaces (e.g., r/ChangeMyView, moderated classrooms). | . | . Conclusion . Pilot wisely, or not at all. FallacyTag can succeed in domains where users already value reflection and are open to structured feedback. It is ill-suited to general-purpose or adversarial platforms without robust customization and explanation layers. Early deployment belongs in the bounded environments: logic-focused subreddits, academic writing seminars, structured peer review systems, internal product retrospectives, or postmortem discussions in engineering teams. These contexts allow careful calibration of tone, mutual expectations, and tolerance for ambiguity. Broader use should wait until interpretability, opt-in mechanics, and user control are robust. FallacyTag isn‚Äôt a switch to flip globally‚Äîit‚Äôs a tool that must prove its value, one careful use case at a time. Without careful scoping, FallacyTag could become not a tool for reflection, but a cudgel‚Äîwielded to shame, score points, or suppress dissent under the guise of reasoning. For builders, the first win isn‚Äôt scale‚Äîit‚Äôs fit. Choose a setting where better reasoning is welcome and small mistakes don‚Äôt poison the well. 7.4 Summary Table . Taken together, these three dimensions‚Äîtechnical, conceptual, and social‚Äîdon‚Äôt just define feasibility; they define the system itself. FallacyTag is not feasible despite its constraints, but because of them. Each boundary (sentence-level detection, hedged labeling, opt-in use) exists to manage a distinct risk: hallucination, misinterpretation, or misuse. When those boundaries align, the system becomes both workable and worthwhile. The table below summarizes feasibility across all three dimensions‚Äîscoring likelihood, surfacing risks, and clarifying key design constraints. | Dimension | Feasibility | Key Risks | Implementation Boundaries | . | Technical | üü¢ High (narrow scope) | Hallucination, ambiguous triggers, noise | Sentence-level inference; fallacy-type filtering; does not generalize to subtle or novel patterns | . | Conceptual | üü† Moderate (boundaries matter) | Fuzzy definitions, rhetorical overlap, cultural variance | Hedged labels; local explanations; avoids deep premise analysis or intent modeling | . | Social/Product | üü° Conditional (context-sensitive) | Misinterpretation, backlash, misuse | Opt-in deployment; interface restraint; unsuitable for adversarial environments without significant adaptation | . 7.5: Scope Boundaries: What FallacyTag Doesn‚Äôt Do . The diagram below outlines what FallacyTag is designed to detect‚Äîand what it deliberately avoids. Figure: FallacyTag Scope Boundary . FallacyTag is explicitly not a fact-checker or moderator. It neither assesses factual correctness nor judges the intent or sincerity behind claims. It focuses strictly on the surface structure of reasoning, detecting recognizable patterns historically associated with reasoning failures. These constraints directly shape the interface: . | Probabilistic tagging produces intentionally hedged cues (e.g., ‚ÄúPossible false dilemma‚Äù) to indicate uncertainty transparently. | Surface structure emphasis ensures tags maintain neutrality, refraining from judgment about intent or correctness | Sentence-level evaluation prevents FallacyTag from speculative or global inference across arguments or contexts. | . Respecting these scope boundaries is critical. Attempting to handle ambiguous fallacies, interpret intent, or verify truthfulness prematurely risks diluting user trust and increasing misinterpretation. Expansion beyond current constraints should proceed only after careful validation within tightly controlled use cases. FallacyTag‚Äôs strength‚Äîand its trustworthiness‚Äîcomes from this careful scoping. The moment it attempts more, it risks accomplishing less. The following section examines how this constraint mindset carries through the system‚Äôs design, especially where stakes are high and ambiguity is fundamental. Previous Next . | Revision Assistant‚Äôs white paper describes ‚ÄúSignal Checks‚Äù that highlight sentence‚Äëlevel strengths and weaknesses tied to rubric-based feedback, visually scaffolded with icons and comments.¬†&#8617; . | Despite its pedagogical promise, Revision Assistant never became a central offering within Turnitin‚Äôs portfolio and appears to have been quietly phased out‚Äîa pattern consistent with known institutional resistance to over‚Äëautomated feedback tools (Inside Higher Ed., 2016, January 21).¬†&#8617; . | IBM‚Äôs Project Debater was engineered to identify claim structures, evidence clusters, and produce speech segments in live debate contexts, demonstrating argument-mining at scale (Roitman et al, 2021).¬†&#8617; . | Coverage of its live debate against champion Harish Natarajan and analysis in The New Yorker highlight that while the system excelled at data-driven argument, it struggled with the rhetorical subtlety, emotional resonance, and ambiguity intrinsic to open discourse.¬†&#8617; . | This mirrors broader insights in explainable AI: what matters most isn‚Äôt model transparency per se, but whether the system offers cues that users can understand and act on. Lipton (2018) argues that interpretability in machine learning is often misunderstood: developers conflate transparency with usability, or mistake post-hoc rationales for ground truth. FallacyTag‚Äôs interface design follows the more pragmatic view: it emphasizes actionable clarity over full model transparency, offering interpretive scaffolding rather than mechanical exposure.¬†&#8617; . | In FallacyTag, ‚Äúprobabilistic tagging‚Äù refers to soft classification based on the model‚Äôs estimated likelihood that a given span matches the structural profile of a fallacy type. These probabilities‚Äîderived from internal confidence scores or auxiliary classifiers‚Äîare not exposed to users numerically. Instead, they shape how tags are presented (e.g., hedged phrasing like ‚ÄúPossible false dilemma‚Äù) and how confidently a tag is surfaced in the UI. Unlike rule-based systems with fixed triggers, FallacyTag relies on surface pattern recognition guided by LLM inference and optionally refined through heuristics or human review. This design prioritizes flexibility and interpretive humility over deterministic output.¬†&#8617; . | Lei &amp; Huang (2024) propose a Logical Structure Tree using constituency parses and connectives to model relations between argumentative spans, modestly boosting LLM-based fallacy detection in structured benchmarks. While theoretically rigorous, these methods require formal parse trees and are optimized for classification pipelines rather than real-time human-facing interaction.¬†&#8617; . | A tag is considered useful not when it proves someone wrong, but when it supports cognitive activation: prompting the user to pause, reconsider structure, or engage more deliberately with counterarguments. In writing contexts, success might look like increased revision of tagged sentences or greater diversity of rhetorical forms in subsequent drafts. In discussions, it might mean that a user edits or qualifies a claim, or simply signals awareness (‚ÄúFair point‚Äù). Even a low engagement rate (e.g., one in five tags sparking reflection) could constitute value if backlash and confusion remain rare.¬†&#8617; . | Many theorists argue that fallacy classification is inseparable from rhetorical context. Tindale (2007) emphasizes that fallacies are best understood as dialogic missteps, not just structural violations, making audience, motive, and genre essential to judgment. Hansen (2023) adds that fallacies often rely on what he calls the ‚Äúappearance condition‚Äù: a poor argument that seems valid in context. Hamblin (1970), meanwhile, exposed a foundational ambiguity in how fallacies are defined‚Äîeither too broad or too narrow to function consistently. FallacyTag sidesteps these tensions. It treats surface patterns as prompts, not verdicts‚Äîan approach aligned with usability over ontological precision.¬†&#8617; . | Research shows that localized, non-evaluative prompts about structure or reasoning‚Äîeven without detailed feedback‚Äîcan improve engagement, revision frequency, and metacognitive awareness. For instance, Roscoe, Allen, and McNamara (2018) found that different tutoring formats influenced motivation and strategy uptake, while Wilson and Roscoe (2020) observed that sentence-level cues increased revision behavior and rhetorical flexibility.¬†&#8617; . | E.g., Teo et al. (2025), Manickavasagam &amp; Bandara (2025).¬†&#8617; . | . ",
    "url": "/fallacytag/pages/07-feasibility/#table-of-contents",
    
    "relUrl": "/pages/07-feasibility/#table-of-contents"
  },"17": {
    "doc": "8. Staying Honest at the Edges",
    "title": "8. Staying Honest at the Edges",
    "content": " ",
    "url": "/fallacytag/pages/08-constraints/",
    
    "relUrl": "/pages/08-constraints/"
  },"18": {
    "doc": "8. Staying Honest at the Edges",
    "title": "Table of Contents",
    "content": ". | 8. Staying Honest at the Edges . | 8.1 Interpretive Ambiguity . | Borderline cases . | Design implication | . | Cultural divergence . | Design implication | . | Genre variation . | Design implication | . | . | 8.2 User Experience and Perception . | Fallacy inflation . | Design implication | . | User control matters . | Design implication | . | Surface simplicity can mislead . | Design implication | . | . | 8.3 Privacy and Consent . | User awareness and control . | Design implication | . | Institutional transparency and configurability . | Design implication | . | . | 8.4 Social Misuse and Weaponization . | Design implication | . | . | . FallacyTag‚Äôs power lies in surfacing argument structure, not in judging ideas. That power vanishes the moment the system feels coercive, overconfident, or culturally misaligned. The constraints in this section aren‚Äôt edge considerations‚Äîthey are the foundation. If FallacyTag fails here, it fails altogether. These constraints shape how the system tags, explains, adapts, and interacts with users‚Äîor fails to. While we present them separately for clarity, they often interact in practice: ambiguity may provoke defensiveness; perceived tone bias can escalate into backlash. 8.1 Interpretive Ambiguity . Reasoning is rarely clean. Many fallacies blur into legitimate heuristics, rhetorical tactics, or cultural idioms. Borderline cases . A forecast might be a slippery slope‚Äîor a prudent extrapolation. A comparison might be a false analogy‚Äîor an accessible metaphor. For example, in a political debate, one candidate claims, ‚ÄúFirst they banned plastic straws‚Äînext they‚Äôll outlaw hamburgers.‚Äù Some see this as a classic slippery slope. Others interpret it as satire or an appeal to regulatory overreach. FallacyTag should flag such statements with hedging and leave interpretation to the user. Design implication . Use soft flags (‚Äúpossible slippery slope?‚Äù) to acknowledge interpretive space. Provide brief local rationales tied to the flagged span, and allow users to expand or dismiss. Cultural divergence . What reads as a fallacy in one discourse community may function as valid shorthand in another. Appeals to collective memory or authority, for instance, may be epistemically valued in certain traditions, especially in religious or ancestral contexts, where they can signal continuity, wisdom, or moral grounding rather than faulty reasoning. While such patterns may remain fallacious in formal terms, flagging them can serve different ends: for some, it may productively challenge inherited assumptions; for others, it may feel dismissive or misaligned with shared norms. In settings where the latter response predominates, visible tagging can backfire, undermining trust not just in the tag, but in the system as a whole. That‚Äôs a hard tradeoff. Suppressing tags in these cases may feel like a loss, but a partially silenced FallacyTag that still earns trust may offer more long-term value than one that overreaches and gets rejected outright. Still, we have to acknowledge that silence carries its own cost: when a community-level configuration disables certain tags, FallacyTag‚Äôs inaction may feel like avoidance‚Äîor even betrayal‚Äîto users who would have welcomed the prompt. Design implication . Support tag customization at the institutional or community level. Allow disabling of specific tag types or adjustment of explanatory tone. Annotators should be tunable, not universal, because imposing a fixed model of logic across all contexts risks epistemic overreach. Genre variation . Satire, protest rhetoric, and speculative fiction routinely violate classical reasoning norms to provoke or reveal more profound truths. Design implication . Consider genre-aware deployment. FallacyTag may not be appropriate for all content types‚Äîits role is clearest in discourses that privilege argumentative structure over affect‚Äîsuch as academic writing, public policy, structured debate, or peer-reviewed commentary. These ambiguities aren‚Äôt exceptions‚Äîthey‚Äôre the norm in everyday discourse. A reasoning tool that ignores them risks becoming brittle, intrusive, or culturally tone-deaf. Interpretive humility isn‚Äôt just principled‚ÄîFallacyTag must be trusted and useful. This isn‚Äôt relativism‚Äîit‚Äôs recognition that reasoning norms vary, and that thoughtful tools must adapt without surrendering structure. Figure ‚Äî Design at the edge of failure. FallacyTag operates in the narrow space between interpretive ambiguity, user perception, and social legitimacy. Push too far in any direction, and the system risks misfire, rejection, or silent complicity. Its only viable path lies in a scoped, transparent, and culturally aware constraint. 8.2 User Experience and Perception . Reasoning aids must be intelligible and trustworthy. Overreach, opacity, or ambiguity in intent can quickly erode user confidence. While Section 7.3 focused on public perception and institutional backlash, this section examines how individual users interpret tags in real time‚Äîand how design choices shape that perception. Fallacy inflation . If tags appear too frequently‚Äîor target marginal phrasing‚Äîusers may begin to ignore them entirely. This erodes trust and undermines the tool‚Äôs signal value. For example, if a tag fires on a casual phrase like ‚ÄúYou always say that‚Äù and flags it as a hasty generalization, users may begin to see the system as pedantic or nitpicky. Design implication . Tags should be sparse and meaningful. Confidence thresholds should be conservative by default, and the system should avoid triggering on stylistic or borderline cases unless explicitly configured otherwise. User control matters . In fast-moving or multilingual settings, users may want to adjust how, when, or whether tags appear. Design implication . Allow toggleable tag visibility, confidence thresholds, or explanatory depth. Ensure accessibility and internationalization are not afterthoughts. Surface simplicity can mislead . Brevity is necessary. FallacyTag must work in fast-moving environments and shouldn‚Äôt bury users in explanation. But compact tags come with risk: a label like ‚ÄúFalse Dilemma‚Äù or ‚ÄúAppeal to Emotion‚Äù can flatten nuance, feel accusatory, or be misread as a verdict‚Äîespecially in high-stakes or politicized settings. Even structurally accurate tags can be interpreted as editorial bias. This is especially true in contexts where users already suspect agenda-driven intervention. If FallacyTag is seen as fact-checker or arbiter, it fails‚Äîregardless of accuracy. Design implication . Provide expandable explanations‚Äîtailored to the specific argument pattern, not just general definitions. Phrase all tag explanations as reasoning cues, not judgments. Use soft framing (‚ÄúThis may suggest‚Ä¶‚Äù) and invite reflection, not correction. 8.3 Privacy and Consent . FallacyTag‚Äôs value rests on trust. If users feel surveilled, judged, or manipulated, adoption will fail, especially in educational or semi-public settings. User awareness and control . Users must know when tagging is active, what data is collected, and how their inputs are used. Hidden telemetry or silent tagging erodes trust, even when technically anonymous. Design implication . Provide opt-out paths where possible. Use local storage and anonymized aggregation for feedback loops. Avoid silent instrumentation. Institutional transparency and configurability . In classrooms or workspaces, tagging must be visible, intelligible, and ethically bounded. Administrators should be able to configure tag types, explanations, and visibility thresholds. End users should see when tagging is active and have the ability to modulate its visibility. Design implication . Require clear disclosure of tagging norms and system behavior. Expose both admin- and user-facing settings, and ensure that configuration is auditable, not hidden. Strong privacy controls help build trust, but they aren‚Äôt enough. Even well-intentioned cues can be misused when social dynamics turn reasoning into performance. That‚Äôs where a second category of risk emerges: weaponization. 8.4 Social Misuse and Weaponization . Even neutral logic cues can be co-opted for mockery, point-scoring, or soft censorship‚Äîespecially in ideological or high-conflict environments. Without guardrails, tags risk becoming performative tools of control rather than aids to reasoning. Design implication . Keep tags private by default. If shared, make tagging visibility part of participatory norms, not unilateral enforcement. In group settings, tagging should support reflection, not policing. This section methodically aligns design implications with FallacyTag‚Äôs core mission of reflection over adjudication, and proposes principled ways to navigate interpretive ambiguity, user perception, privacy, and social misuse. However, the assessment also acknowledges that many of these ‚Äúedges‚Äù are managed rather than fully solved. The efficacy of FallacyTag‚Äôs proposed solutions ultimately depends heavily on user adoption, accurate interpretation, and the willingness of communities to foster norms that prioritize constructive reasoning over adversarial engagement, a social challenge that we view as conditional in general public contexts (see Section 7.3 and Appenix C). FallacyTag‚Äôs legitimacy rests on respecting constraints‚Äîinterpretive, social, and perceptual. Yet even a carefully bounded tool raises an important question: When is it actually worth deploying? Constraints alone don‚Äôt answer that; they only frame the boundaries of possibility. Next, we propose a structured way to determine which contexts genuinely invite FallacyTag‚Äîand which are better left untouched. Previous Next . ",
    "url": "/fallacytag/pages/08-constraints/#table-of-contents",
    
    "relUrl": "/pages/08-constraints/#table-of-contents"
  },"19": {
    "doc": "9. Conditional Case - When This Is Worth Building",
    "title": "9. Conditional Case: When This Is Worth Building",
    "content": " ",
    "url": "/fallacytag/pages/09-worth-building/#9-conditional-case-when-this-is-worth-building",
    
    "relUrl": "/pages/09-worth-building/#9-conditional-case-when-this-is-worth-building"
  },"20": {
    "doc": "9. Conditional Case - When This Is Worth Building",
    "title": "Table of Contents",
    "content": ". | 9. Conditional Case: When This Is Worth Building . | 9.1 Dirac, Scorecards, and the Dangers of Numbers | 9.2 What the Scorecard Is and Isn‚Äôt | 9.3 The Scorecard . | 9.3.1 Dimensions . | 9.3.1.1 Independence &amp; Weighting | . | 9.3.2 Interpreting Scores | . | 9.4 Use Cases in Context . | 9.4.1 Structured Peer Feedback in a College Writing Course | 9.4.2 AI Chatbot Conversation with FallacyTag Overlay | 9.4.3 Postmortem Docs in a Software Engineering Org | . | 9.4.4 ü§°: AI Dungeon with FallacyTag | 9.4.5¬†Podcast Player with Fallacy-Tagged Transcript View . | What would need to be true? | . | 9.4.6 YouTube Comments on a Political Explainer . | What would need to be true? | . | 9.5 Reflection: Why These Examples Matter . | Why Offer Rescue Paths? | . | 9.6 Summary: Worth, With Constraints | . | . FallacyTag is not a neutral feature‚Äîit introduces friction, scaffolding, and risk. We need a principled way to decide where it belongs and where it doesn‚Äôt. Not every context that values clarity is ready for structural tagging. This section offers a framework. And yes, that framework involves a scorecard. 9.1 Dirac, Scorecards, and the Dangers of Numbers . Physicist Paul Dirac famously hated small talk. At a dinner party, someone once teased him: ‚ÄúWe hear you don‚Äôt talk much, Dr. Dirac. Why is that?‚Äù Dirac replied: ‚ÄúBecause words are dangerous. They can be misused.‚Äù . Now, if your spidey sense is tingling‚Äîgood. The same chatbot that dreamed up this exchange also argued strenuously against giving you another manipulative instrument: a scorecard. It kept proposing softer names (‚ÄúFit Gauge,‚Äù ‚ÄúReadiness Lens‚Äù), replacing numbers with emojis, and otherwise trying to protect you from the risk of being misled‚Äîrisks like: . | False precision ‚Äì numbers look more objective than they are. | Pass/fail thinking ‚Äì totals invite binary ‚Äúgood/bad‚Äù judgments. | Aggregate blindness ‚Äì a single score can hide very different underlying patterns. | Weaponization ‚Äì numbers get cherry-picked to justify decisions. | Shortcutting nuance ‚Äì people skip the explanation and look at the score. | . And yet here we are. Why stoop so low? Because you probably want it. Your supervisor (if you are lucky to have one) almost certainly does. And, deep down, I‚Äôm hoping the scorecard spits out something unexpected‚Äîsomething that makes you think. I trust you to see through the emojis, the naming gymnastics, and the polite evasions. I also trust you not to harm yourself with it. 9.2 What the Scorecard Is and Isn‚Äôt . The scorecard is not a truth oracle. It doesn‚Äôt model human behavior, politics, or the psychology of defensiveness. It won‚Äôt predict adoption rates or guarantee utility. But it can do something narrower and still useful: . It helps surface design-relevant characteristics of your intended context‚Äîbefore you build. By scoring across a few key dimensions, it encourages reflection: What kind of reasoning happens here? How hostile is the tone? Would users appreciate this nudge, or resist it? The goal is not precision, but structured judgment. Two use cases might score the same overall for entirely different reasons‚Äîthat‚Äôs the point. This is a profile, not a thermometer. 9.3 The Scorecard . The scorecard is a pre-deployment litmus test: a structured prompt for assessing whether an environment is cognitively, socially, and logistically compatible with lightweight, sentence-level reasoning cues. It doesn‚Äôt capture everything, but it flags what matters most. 9.3.1 Dimensions . Each environment is scored across seven dimensions‚Äînot for detection accuracy or engineering quality, but for environmental fit. These dimensions are conceptually defined, but scoring works best when anchored in what can be observed. The table below pairs each definition with a plain-language question (‚ÄúWhat it asks‚Äù) and concrete indicators (‚ÄúWhat to look at / Evidence‚Äù) to make evaluation practical. Each is scored 0‚Äì2: . | 2 = Clear alignment with FallacyTag‚Äôs design constraints | 1 = Mixed; could work with care, or in sub-contexts | 0 = High risk of harm, misfit, or irrelevance | . 9.3.1.1 Independence &amp; Weighting . We score each dimension independently for several reasons: . | Avoid double-counting ‚Äî Overlaps between dimensions can inflate scores and mask weaknesses. | Different observability and mitigation paths ‚Äî Some dimensions are easier to measure or address than others, so we want to adjust them without recalculating others. | Correlations exist but are ignored for scoring ‚Äî Dimensions often reinforce each other (e.g., strong ‚ÄúDiscourse Structure‚Äù can boost ‚ÄúClarity of Claims‚Äù). Still, we treat them as separate to keep the rubric simple and usable without statistical modeling. | . We also use equal weighting: . | This avoids meta-debates over which dimensions ‚Äúmatter more‚Äù and keeps the focus on the evidence for each score. | Equal weighting here doesn‚Äôt mean all dimensions are equally important ‚Äî their salience varies by context, and our confidence in measuring them also varies. | . See Appendix E for the detailed rationale, examples, and possible refinements. 9.3.2 Interpreting Scores . This isn‚Äôt a metric. It‚Äôs a lens. Scores don‚Äôt dictate decisions‚Äîthey highlight tension. A low number doesn‚Äôt mean ‚Äúdon‚Äôt deploy,‚Äù just as a high one doesn‚Äôt mean ‚Äúgo ahead.‚Äù What matters is why an environment scores the way it does‚Äîand whether those constraints can be designed around. Use the scorecard to surface risk, clarify tradeoffs, and sharpen judgment. Treat it less like a checklist and more like a pre-mortem: where might this go wrong, and why? . Interpret as prompts, not pass/fail: . This helps shift the question from ‚ÄúCan we do this?‚Äù to something more precise: . ‚ÄúDoes this environment invite the kind of reasoning support FallacyTag is built to offer?‚Äù . The following section puts this into practice with a set of representative use cases. Some score high, some fail dramatically‚Äîand each tells us something about what ‚Äúfit‚Äù really means. 9.4 Use Cases in Context . Scorecards are only as useful as the conversations they provoke. To ground the dimensions in real environments, we now apply them to a range of representative use cases. These are not endorsements or rejections. They‚Äôre sketches‚Äîeach one intended to expose how environmental characteristics shape the viability of reasoning feedback. This is also where the focus begins to shift. Much of the prior work on fallacy detection‚Äîincluding Lei &amp; Huang‚Äôs Logical Structure Tree (2024) and Sourati et al.‚Äôs Tree of Fallacies framework (2023)‚Äîhas concentrated on representing and classifying flawed reasoning12. These contributions provide rich internal structures and have influenced the thinking behind this project. Yet, while they sometimes note possible applications, they rarely engage deeply with the deployment context: how, where, and for whom such feedback should be surfaced in practice. The following use cases aim to extend that conversation. They treat fit and fragility as first-class concerns, exploring not just what a system like FallacyTag can detect, but whether the environment invites it to speak‚Äîand whether anyone‚Äôs likely to listen. 9.4.1 Structured Peer Feedback in a College Writing Course . Context: Students use a learning platform (e.g., Eli Review, Canvas) to review each other‚Äôs essays. FallacyTag surfaces inline cues during feedback or revision. Total: 12/14 . üü¢ Verdict: A strong early fit. Minor concerns around control and social signaling, but these are manageable in educational settings. Tags should be visible only to the author and instructor. 9.4.2 AI Chatbot Conversation with FallacyTag Overlay . Context: A user interacts with an AI chatbot via the standard web interface. A FallacyTag browser extension runs locally, surfacing inline reasoning flaws in the model‚Äôs outputs (not the user‚Äôs prompts). Total: 12/14 . üü¢ Verdict: Strong candidate. Tagging LLM output gives users a reasoning ‚Äúsecond opinion‚Äù in real time. Especially valuable in contexts like news, health, or technical explanation, where the user is not just browsing but evaluating. Care must be taken to clarify that FallacyTag is reflecting on model logic, not user behavior. Still, as a low-friction tool for reflective interaction, it fits almost perfectly. 9.4.3 Postmortem Docs in a Software Engineering Org . Context: Teams write retrospectives after production incidents. FallacyTag runs on the doc platform to highlight structural issues in reasoning (e.g., causal leaps, scapegoating). Total: 14/14 . üü¢ Verdict: Nearly ideal. Cultural norms already support structured thinking and low-ego critique. 9.4.4 ü§°: AI Dungeon with FallacyTag . This use case is unlike the others. It‚Äôs here in small part because a bad fit made apparent can teach as much as a good one, but mostly to entertain. Skip this section if you are not in the mood. Context: Interactive fiction game where users co-create absurdist narratives with an AI language model. FallacyTag is applied to player inputs and in-game character responses. Total: 7/14 . üü° Verdict: The system works. The user does not. FallacyTag here becomes meta-humor or misapplied rigor‚Äîa parody of itself. But that‚Äôs the point: not all flawed logic wants fixing. This case reveals what the viable ones have that this doesn‚Äôt: stakes, structure, and shared intention. 9.4.5¬†Podcast Player with Fallacy-Tagged Transcript View . Context: A mobile app offers transcripts of podcasts. FallacyTag adds optional timestamped reasoning cues for reflection post-listening. Total: 7/14 . üü° Verdict: Borderline viable in niche settings (e.g., academic or science podcasts). Mass-market rollout would likely underperform or backfire. What would need to be true? . For this to work, podcast content would need to be consistently reflective‚Äîe.g., academic interviews, structured debates, or narrative explainers with defined argument arcs. More importantly, tag delivery must shift from real-time interruption to post-listening summary. A companion app that gently surfaces structural cues after the fact‚Äîperhaps with searchable timestamps‚Äîmight preserve flow while still offering insight. Pairing this with user-curated moments (‚Äúmark this for deeper review‚Äù) could foster reflective uptake. 9.4.6 YouTube Comments on a Political Explainer . Context: A browser extension overlays FallacyTag annotations on YouTube comment threads below a video about climate policy. Total: 3/14 . üî¥ Verdict: Almost certainly counterproductive. The surface is too hostile, the control too weak, and the risk of misinterpretation too high. What would need to be true? . Almost everything. First, tagging visibility would need to be opt-in and personal‚Äîa viewer-only lens rather than a public overlay. Second, the comment environment would need moderation norms already in place that favor reasoning over rhetoric. A better fit might be structured Q&amp;A beneath educational videos or small-group classroom discussion portals using YouTube embeds. Even then, tagging should apply to content, not commenters. Failing that, the only viable angle here is meta‚Äîusing the chaos of the comment thread itself as a teaching exhibit: ‚ÄúWhat tags would you apply to this exchange, and why?‚Äù That‚Äôs pedagogy, not product. 9.5 Reflection: Why These Examples Matter . These examples show what the scorecard can‚Äôt: how the same system can feel elegant in one space and illegitimate in another. It‚Äôs not just about who the users are‚Äîit‚Äôs about what the environment invites. FallacyTag doesn‚Äôt demand ideal conditions. But it needs conditions that don‚Äôt actively reject what it offers: clarity, humility, and structural reflection. Why Offer Rescue Paths? . Scorecards don‚Äôt predict failure‚Äîthey illuminate fit. Even low-scoring environments might support FallacyTag‚Äîbut only with strong guardrails, clever reframing, or context-aware constraint. The question shifts from ‚ÄúCan we deploy here?‚Äù to ‚ÄúWhat would it take to make this work‚Äîand is it worth it?‚Äù . 9.6 Summary: Worth, With Constraints . FallacyTag isn‚Äôt a grand vision. It‚Äôs a bounded intervention. Its value doesn‚Äôt lie in universality‚Äîit lies in fit. Section 9 laid out that fit through three layers: . | Why does this kind of system demand special justification (9.0‚Äî9.2), | How to evaluate environmental compatibility via the scorecard (9.3), | Where that framework breaks open meaningful differences (9.4). | . What emerges is not a binary. FallacyTag isn‚Äôt ‚Äúappropriate‚Äù or ‚Äúinappropriate‚Äù‚Äîit‚Äôs context-sensitive, socially fragile, and cognitively demanding. But when the setting aligns‚Äîwhen users are present to reason, when the tone allows gentle nudges, when control is visible and earned‚Äîit can work. It can help. And that‚Äôs enough. Designing for reflection, rather than judgment, is rare in software. Rarer still is designing with structural humility. FallacyTag invites both. The rest of this paper‚Äîarchitecture, tone, constraints‚Äîis built to preserve that stance. Previous Next . | Lei, S., &amp; Huang, D. (2024) extract argument structure into a ‚Äúlogical structure tree‚Äù to guide LLM classification, yielding higher accuracy on benchmark datasets.¬†&#8617; . | Sourati, Z., et al. (2023) present an explainable classification pipeline for multiple fallacy types, evaluated on curated datasets, with limited discussion of user-facing integration.¬†&#8617; . | . ",
    "url": "/fallacytag/pages/09-worth-building/#table-of-contents",
    
    "relUrl": "/pages/09-worth-building/#table-of-contents"
  },"21": {
    "doc": "9. Conditional Case - When This Is Worth Building",
    "title": "9. Conditional Case - When This Is Worth Building",
    "content": " ",
    "url": "/fallacytag/pages/09-worth-building/",
    
    "relUrl": "/pages/09-worth-building/"
  },"22": {
    "doc": "10. Call to Explore",
    "title": "10. Call to Explore",
    "content": "Coming soon‚Ä¶ . Previous . ",
    "url": "/fallacytag/pages/10-call-to-explore/",
    
    "relUrl": "/pages/10-call-to-explore/"
  },"23": {
    "doc": "Appendix A - Logic Check Experiment: Why Democracy Is Mathematically Impossible by Veritasium",
    "title": "Appendix A - Logic Check Experiment: Why Democracy Is Mathematically Impossible by Veritasium",
    "content": " ",
    "url": "/fallacytag/pages/appendices/appendix-a/",
    
    "relUrl": "/pages/appendices/appendix-a/"
  },"24": {
    "doc": "Appendix A - Logic Check Experiment: Why Democracy Is Mathematically Impossible by Veritasium",
    "title": "Table of Contents",
    "content": ". | Appendix A - Logic Check Experiment: Why Democracy Is Mathematically Impossible by Veritasium . | Source | Case Study: (Inadvertently) Scaffolded Prompt . | üîç Prompt | ü§ñ GPT Response | . | . | . Source . https://youtu.be/qf7ws2DF-zk . Case Study: (Inadvertently) Scaffolded Prompt . üîç Prompt . I‚Äôd like to discuss the logical quality of the conclusion of a YouTube transcript. This is the core of the argument, without conclusion: . ‚Ä¶ (Ommitted for brevity; transcript remains available on Youtube) ‚Ä¶ . Now the conclusion: . ‚ÄúSo is democracy mathematically impossible? Well, yes, if we use ranked choice methods of voting, which is what most countries in the world use to elect their leaders. And some methods are clearly better at aggregating the people‚Äôs preferences than others, the use of first past the post voting feels quite frankly ridiculous to me, given all of its flaws. But just because things aren‚Äôt perfect doesn‚Äôt mean we shouldn‚Äôt try. Being interested in the world around us, caring about issues, and being politically engaged is important. It might be one of the few ways we can make a real difference in the world. Like Winston Churchill said, ‚ÄòDemocracy is the worst form of government except for all the other forms that have been tried.‚Äô Democracy is not perfect, but it‚Äôs the best thing we‚Äôve got. The game might be crooked, but it‚Äôs the only game in town.‚Äù . Does the conclusion actually follow from the previous content? Is it logical? Do you detect specific fallacies? . ü§ñ GPT Response . The unabridged response is lost in the ether, but the exact content that impressed me, manually organized in a single table (copy/paste only), follows: . | Fallacy Name | Explanation in Context of the Transcript Conclusion | . | Equivocation | The word ‚Äúdemocracy‚Äù is used in two different senses: (1) a political system of collective decision-making, and (2) a voting method that aggregates individual ranked preferences. The conclusion claims ‚Äúdemocracy is mathematically impossible‚Äù based on the second meaning, while implying a broader political critique using the first. | . | Category Error | Arrow‚Äôs theorem demonstrates limits of preference aggregation mechanisms, not of democracy as a political institution. The conclusion incorrectly treats a limitation in one technical component as a flaw in the overall concept. | . | Appeal to Authority / Sentiment | The Churchill quote (‚ÄúDemocracy is the worst form‚Ä¶‚Äù) is invoked to justify staying engaged with democracy, but it bypasses logical argumentation. Churchill is not a mathematical authority, and his aphorism is used to close a technical argument emotionally, not analytically. | . | Non Sequitur | The conclusion jumps from ‚Äúperfect preference aggregation is impossible‚Äù to ‚Äútherefore, we should stay politically engaged.‚Äù This leap is not logically justified by the body of the argument, which focused narrowly on voting system mechanics, not civic virtue or political participation. | . ",
    "url": "/fallacytag/pages/appendices/appendix-a/#table-of-contents",
    
    "relUrl": "/pages/appendices/appendix-a/#table-of-contents"
  },"25": {
    "doc": "Appendix B - Logic Check Experiment: Reddit CMV on Local vs National Politics",
    "title": "Appendix B - Logic Check Experiment: Reddit CMV on Local vs National Politics",
    "content": " ",
    "url": "/fallacytag/pages/appendices/appendix-b/",
    
    "relUrl": "/pages/appendices/appendix-b/"
  },"26": {
    "doc": "Appendix B - Logic Check Experiment: Reddit CMV on Local vs National Politics",
    "title": "Table of Contents",
    "content": ". | Appendix B - Logic Check Experiment: Reddit CMV on Local vs National Politics . | Source | üîç Prompt (used in all three cases) | Case 1 . | üí¨¬†Comment | ü§ñ GPT Response | . | Case 2 . | üí¨¬†Comment | ü§ñ GPT Response | . | Case 3 . | üí¨¬†Comment | ü§ñ GPT Response | . | . | . Source . r/ChangeMyView CMV: Local politics is much more important to your daily life than national politics . üîç Prompt (used in all three cases) . Identify any flawed reasoning and explain why: . (comment text) . Case 1 . üí¨¬†Comment . Would you say the difference between two different towns of the same size in your state is more or less significant than the difference between your town and the most similar Canadian analog? Because it seems to me that one town‚Äôs very like another when your head‚Äôs down over your pieces, brother whereas countries are much more strongly different from one another. Because national politics really varies much more sharply. In theory local politics could vary more sharply but in practice it doesn‚Äôt because towns and cities mostly just copy one another and if they do try to do crazy stuff people move away. ü§ñ GPT Response . This passage contains several instances of flawed reasoning, particularly in terms of overgeneralization, false equivalence, and unsupported assumptions. Here‚Äôs a breakdown: . 1. Overgeneralization: . ‚ÄúTowns and cities mostly just copy one another‚Ä¶‚Äù . | This is an overgeneralization. While towns may adopt similar policies due to shared challenges, many diverge meaningfully in leadership, culture, and priorities. | . 2. False Dichotomy / False Equivalence: . ‚ÄúOne town‚Äôs very like another‚Ä¶ whereas countries are much more strongly different‚Ä¶‚Äù . | The comparison oversimplifies. Some Canadian towns may resemble American towns more than other American towns do. | . 3. Unsupported Causal Claim: . ‚Äú‚Ä¶if they do try to do crazy stuff people move away.‚Äù . | People move for many reasons; the claim assumes too direct a relationship between local policy and migration. | . 4. Appeal to Subjective Experience: . ‚Äú‚Ä¶when your head‚Äôs down over your pieces, brother‚Ä¶‚Äù . | Rhetorically colorful, but relies on personal sentiment rather than logic or data. | . Conclusion: The reasoning overstates national variation and downplays meaningful local differences through generalizations and untested assumptions. Case 2 . üí¨¬†Comment . Local politics define if there‚Äôs some bollards or speed bumps on my street. National politics decide if I have healthcare, how much I pay for university and if I can buy a house someday or not. ü§ñ GPT Response . üí° . The statement draws a reasonable contrast but contains oversimplifications and false dichotomy. Key issues include: . 1. False Dichotomy / Oversimplification: . | It implies only federal politics matter for big issues. In fact: | Housing affordability is heavily influenced by local zoning. | University costs often depend on state funding. | Healthcare access is shaped by both federal and state policy. | . 2. Implied Value Judgment: . | It subtly dismisses local politics as trivial (‚Äúspeed bumps‚Äù), ignoring important local functions like schooling, transit, and public safety. | . 3. Selective Framing: . | The examples cherry-pick federal power while ignoring where local control is dominant. | . Conclusion: While the federal government shapes many broad systems, the argument overstates its exclusivity and underrepresents local impact on life outcomes. Case 3 . üí¨¬†Comment . No one‚Äôs ever been drafted by their state or county government. Federal taxes are much higher than local. Teaching to the test is exacerbated by W‚Äôs No Child Left Behind bill. Many local road projects are majority funded by federal funds, even more so for highways. Almost everyone will come to depend on Medicare and Social Security at some point in their lives. ü§ñ GPT Response . The comment builds a case for federal dominance in influence, but relies on selective evidence, scope neglect, and equivocation. 1. Cherry-Picking: . | It highlights federal powers (e.g., draft, taxes, entitlements) while ignoring where state/local governments are deeply consequential (e.g., police, schools, housing). | . 2. Scope Neglect: . ‚ÄúFederal taxes are much higher‚Ä¶‚Äù . | Often true, but ignores that local taxes fund vital, visible services (schools, water, fire). | . 3. Oversimplification: . ‚ÄúTeaching to the test is exacerbated by No Child Left Behind‚Ä¶‚Äù . | True to an extent, but the law was interpreted and implemented differently by each state, and local schools determined how to meet test mandates. | . 4. Equivocation on ‚Äòimportance‚Äô: . | It conflates scale of policy with impact on daily life. For many, local decisions (e.g., crime, housing, school quality) are just as impactful as national ones. | . Conclusion: The argument rests on a one-sided portrayal of federal authority, minimizing the real power and variability of local governance. ",
    "url": "/fallacytag/pages/appendices/appendix-b/#table-of-contents",
    
    "relUrl": "/pages/appendices/appendix-b/#table-of-contents"
  },"27": {
    "doc": "Appendix C: On Learning from User Feedback",
    "title": "Appendix C: On Learning from User Feedback",
    "content": "B.1 The Intuition . FallacyTag invites feedback. When a user marks a tag as ‚Äúincorrect,‚Äù that judgment is recorded. In theory, such input could help the system improve‚Äîby refining detection strategies, adjusting thresholds, or adapting to context over time. The intuition is familiar: learn from disagreement. But in practice, learning from this kind of feedback is much harder than it looks. B.2 Why Feedback Is Ambiguous . Not all rejections are equal‚Äîand not all are about reasoning. A user might dismiss a tag because: . | It feels wrong for the tone or genre (e.g., casual podcast vs. formal debate) | They don‚Äôt like the interruption | They agree with the conclusion, and mistake critique for contradiction | . In reasoning contexts, the line between ‚Äúthis is a fallacy‚Äù and ‚Äúthis rubs me the wrong way‚Äù is thin. Even sincere feedback can mislead if the signal is entangled with sentiment, ideology, or attention mode. B.3 Social Bias and Tribal Rejection . Beyond ambiguity, there‚Äôs distortion. In public contexts, feedback is shaped by: . | Confirmation bias ‚Äî users tend to protect reasoning they agree with | Social signaling ‚Äî rejections often reflect tribal alignment, not logical appraisal | Moral loading ‚Äî tags may be read as ideological attacks, not structural cues | . In polarized environments, these effects can dominate. Tags get rejected not because they‚Äôre wrong, but because they‚Äôre unwelcome. In such settings, FallacyTag may not be misfiring‚Äîit may simply be misfit. B.4 Strategies Considered (and Their Limits) . We explored multiple feedback-driven improvement strategies: . | Strategy | Idea | Limitation | . | Rejection Clustering | Flag tags that get dismissed often | May reinforce local bias or majority opinion | . | Contextual Caching | Tune logic by media format or genre | Requires strong priors; risks overfitting | . | Prompt Chain Feedback | Feed rejections back into generation chains | Adds complexity without clear gains | . | Revision Tracing | Look for edits following dismissals | Sparse and difficult to correlate with feedback meaningfully | . | Cohort Modeling | Segment users by behavior for weighting | Requires scale and raises privacy/framing questions | . Each approach offered a partial signal. None yielded a robust, general-purpose improvement loop. B.5 Why the Loop Is Open‚ÄîFor Now . FallacyTag currently stores feedback but does not act on it. This is intentional. In a system designed to support reasoning‚Äînot just user preference‚Äîlearning from ambiguous or adversarial signals may do more harm than good. The system might shift toward comfort, not clarity. The right path may involve: . | Better modeling of user intent | New interaction patterns that disambiguate feedback | Feedback that arises from editing behavior, not just button taps | . Until then, restraint is a feature, not a limitation. The system listens‚Äîbut it does not contort. ",
    "url": "/fallacytag/pages/appendices/appendix-c/",
    
    "relUrl": "/pages/appendices/appendix-c/"
  },"28": {
    "doc": "Appendix D: FallacyTag Variability Model",
    "title": "Appendix D: FallacyTag Variability Model",
    "content": " ",
    "url": "/fallacytag/pages/appendices/appendix-d/",
    
    "relUrl": "/pages/appendices/appendix-d/"
  },"29": {
    "doc": "Appendix D: FallacyTag Variability Model",
    "title": "Table of Contents",
    "content": ". | üîß Variation Points Summary Table | üß≠ Feature Hierarchy Diagram | üìå Usage | . This appendix presents a structured view of the FallacyTag architecture as a product line architecture (PLA), capturing its key variation points, available options, and constraints. It supports implementers in identifying viable configurations and understanding architectural trade-offs. üîß Variation Points Summary Table . | Variation Point | Options | Constraints | Notes | . | Privacy Handling (Privacy ‚Üî Observability) | Local-only, Session-based, Identity-linked | Must preserve user autonomy; no profiling; minimal friction | Options vary in how much data persists and travels: ‚Äúlocal-only‚Äù retains nothing, ‚Äúidentity-linked‚Äù enables long-term telemetry, requiring strong safeguards. | . | Inference Type (Statelessness ‚Üî Adaptivity) | Prompt-only, Session-adaptive, Fine-tuned w/ memory | Must preserve auditability and avoid unpredictable drift | ‚ÄúPrompt-only‚Äù provides maximal transparency. ‚ÄúFine-tuned‚Äù risks subtle shifts. Session-adaptive sits in between, retaining state temporarily. | . | Tag Strategy (Interpretability ‚Üî Expressive Power) | Symbolic (sentence-level), Discourse-aware, Learned patterns | Must maintain interpretability; minimize hallucination | ‚ÄúSymbolic‚Äù uses explicit templates and rule-based mappings. ‚ÄúLearned patterns‚Äù are more flexible but harder to explain. | . | UI Integration (Separation ‚Üî Intrusion) | Post-hoc Review, Inline Overlay, Export-Only | Must preserve primary experience flow; avoid disruptive or adversarial UX | ‚ÄúPost-hoc‚Äù works best in slower or asynchronous modes; ‚Äúinline‚Äù suits text; ‚Äúexport-only‚Äù fits detached workflows like podcasts. | . | Feedback Use | None, Aggregate Telemetry, User-linked Adaptation | Feedback must not affect inference by default; linkage to identity must be explicitly authorized | Intersects both privacy and adaptivity: feedback pipelines can erode statelessness and compromise anonymity. Aggregate telemetry enables improvement without retraining or profiling. | . | Downstream Use (Reflection ‚Üî Enforcement Drift) | Reflective tagging, Optional annotations, Scoring/moderation | Must avoid authoritative or punitive use; system intent must remain transparent | Tags may be reused for judgment or ranking downstream, even if designed for reflection. Implementers must guard against misinterpretation of system posture. | . üß≠ Feature Hierarchy Diagram . üìå Usage . This model provides implementers and researchers with a structured way to reason about system variants. It complements the prose discussion in Section 6.2 and is suitable for: . | Evaluating architectural fit for a target domain (e.g., public discourse vs. classrooms) | Documenting design decisions and their implications | Supporting modular implementation planning | . Future work may include mapping these features to implementation modules or tracing variation point dependencies. ",
    "url": "/fallacytag/pages/appendices/appendix-d/#table-of-contents",
    
    "relUrl": "/pages/appendices/appendix-d/#table-of-contents"
  },"30": {
    "doc": "Appendix E - Independence, Correlation, and Weighting",
    "title": "Appendix E - Independence, Correlation, and Weighting",
    "content": " ",
    "url": "/fallacytag/pages/appendices/appendix-e/",
    
    "relUrl": "/pages/appendices/appendix-e/"
  },"31": {
    "doc": "Appendix E - Independence, Correlation, and Weighting",
    "title": "Table of Contents",
    "content": ". | Why Independence Matters | Known Correlations | Why Equal Weighting | Confidence in Measurement | Possible Future Refinements | . The seven dimensions in the scorecard are deliberately scored independently and given equal weight in the overall total. This choice reflects a trade-off between methodological purity and practical usability. Why Independence Matters . Treating dimensions as independent forces the evaluator to consider each on its own merits. This: . | Reduces double-counting ‚Äî Without separation, a single strength (e.g., precise discourse) might indirectly inflate scores in related areas (e.g., clarity of claims), creating a falsely positive profile. | Highlights weak links ‚Äî An excellent showing in five dimensions doesn‚Äôt obscure the fact that one or two are inadequate. | Allows targeted mitigation ‚Äî Some dimensions are more readily observable or controllable. For example: . | User Control &amp; Transparency can often be addressed via deployment choices. | Tag Interpretability depends more on shared scaffolding or onboarding. | . | . Treating them independently allows for direct intervention where needed without recalculating the whole profile. Known Correlations . In practice, some dimensions will be statistically related. Examples: . | Strong Discourse Structure often supports Clarity of Claims. | User Intent and Tone &amp; Stakes can influence each other in high-conflict settings. | . We treat these as conceptually distinct despite overlap, for two reasons: . | Correlation is not causation ‚Äî a strength in one dimension doesn‚Äôt guarantee adequacy in another. | Correlation strength varies widely by context (e.g., formal debate vs. casual group chat). | Keeping them separate simplifies application and avoids the need for statistical modeling in deployment. | . Why Equal Weighting . Weighting dimensions according to perceived importance can, in theory, produce a more ‚Äúaccurate‚Äù measure. In practice, it often: . | Invites meta-debates ‚Äî Discussions drift toward whether ‚ÄúTone &amp; Stakes‚Äù deserves more weight than ‚ÄúUser Control &amp; Transparency‚Äù instead of focusing on the evidence for each score. | Adds false precision ‚Äî Assigning, say, 1.5√ó to a dimension can imply a level of measurement accuracy we don‚Äôt actually have. | . Our choice to weight equally is deliberate, not because all dimensions are equally important, but because: . | Their salience is context-dependent (e.g., Modality Fit may be critical for YouTube, less so for text forums). | The goal is to provoke discussion, not enforce priorities. | We want a usable default that minimizes subjective weighting disputes. | . Confidence in Measurement . Scoring confidence varies by dimension: . | Some scores reflect concrete, easily-verified conditions (Is the input text?). | Others require contextual judgment (Is reasoning the goal?). | . This hybrid nature reflects the tool‚Äôs role: not a precise readiness metric, but a structured prompt for reflective deployment. Possible Future Refinements . If the scorecard were deployed at scale and large, high-quality datasets were available, statistical analysis could inform: . | Adjusted weights to better predict successful deployments. | Clustering or factor analysis to identify which dimensions most strongly co-vary in practice. | Context-specific weight sets (e.g., separate for synchronous vs. asynchronous environments). | . Until then, independence and equal weighting are pragmatic defaults: they minimize complexity, reduce bias from subjective weighting decisions, and keep the tool accessible to non-technical evaluators. ",
    "url": "/fallacytag/pages/appendices/appendix-e/#table-of-contents",
    
    "relUrl": "/pages/appendices/appendix-e/#table-of-contents"
  },"32": {
    "doc": "Appendices",
    "title": "Appendices",
    "content": "This section collects supplementary material, extended analysis, and design reflections referenced throughout the main text. While not required for core understanding, these appendices offer deeper insight into decisions made during the development of FallacyTag. | Appendix A is the transcript of GPT-4‚Äôs logical check of Veritassium‚Äôs ‚ÄúWhy Democracy Is Mathematically Impossible‚Äù video. | Appendix B is the transcript of GPT-4‚Äôs logical check of a few posts on Reddit CMV on Local vs National Politics. | Appendix C discusses the challenges and limitations of using user feedback to improve the system. | Appendix D presents a structured view of the FallacyTag architecture as a product line architecture (PLA) | Appendix E explains why the seven scorecard dimensions are scored independently and weighted equally. It discusses the risks of double-counting and meta-debates, acknowledges natural correlations, and outlines when more sophisticated weighting might be justified in future deployments. | | . ",
    "url": "/fallacytag/pages/appendices/appendix-home/",
    
    "relUrl": "/pages/appendices/appendix-home/"
  },"33": {
    "doc": "References",
    "title": "References",
    "content": "Coming soon‚Ä¶ . ",
    "url": "/fallacytag/pages/references/",
    
    "relUrl": "/pages/references/"
  }
}
