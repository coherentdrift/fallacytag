{"0": {
    "doc": "Hello",
    "title": "FallacyTag: Surfacing Argument Structure in Public Discourse",
    "content": "A lightweight reasoning-feedback layer for flawed logic in the wild. 📬 Subscribe on Substack 🧪 Live interactive demo . FallacyTag is an experiment in offering structure to public reasoning. It explores how we might give flawed arguments the same visibility as typos— not to shame, but to support reflection, clarity, and better conversations. This site hosts the live draft of the FallacyTag paper. Sections are being released incrementally. So far, the following are available: . | ✅ 0: Abstract | ✅ 1: Introduction | ✅ 2: The Architecture of Avoidance | ✅ 3: Scaffolded Intelligence: How LLMs Can Support Better Reasoning | ✅ 4: FallacyTag: A Gentle Loop for Sharper Thinking | ✅ 5: Mediums, Modalities and Fit | ⏳ Sections 6–10: Coming soon | . Check the sidebar for what’s currently available, or follow the project on Substack for updates. If you’re a builder, educator, or curious skeptic—welcome. ",
    "url": "/fallacytag/#fallacytag-surfacing-argument-structure-in-public-discourse",
    
    "relUrl": "/#fallacytag-surfacing-argument-structure-in-public-discourse"
  },"1": {
    "doc": "Hello",
    "title": "Hello",
    "content": " ",
    "url": "/fallacytag/",
    
    "relUrl": "/"
  },"2": {
    "doc": "0. Abstract",
    "title": "0. Abstract",
    "content": "FallacyTag is a lightweight reasoning aid designed to surface flawed argument structure in online discourse—gently, interpretably, and without moralizing. Built around modern LLM capabilities and constrained design, it offers sentence-level feedback in the form of informal fallacy cues: not judgments, but structural prompts. This paper presents the concept, sketches its architecture, explores its feasibility across technical, social, and conceptual dimensions, and proposes a framework for deployment in varied modalities (text, audio, video). The core bet is simple: by making reasoning structure visible—even imperfectly—we can foster better reflection without requiring cultural revolutions or cognitive overhauls. FallacyTag isn’t a moderator or a fact-checker. It’s a nudge toward clarity, designed to fit where logic matters and tone is fragile. The paper is exploratory in form, rigorous in ambition, and deliberately uneven in tone. It is meant as a public proposal—for critique, adaptation, or reinvention. Next . ",
    "url": "/fallacytag/pages/00-abstract/",
    
    "relUrl": "/pages/00-abstract/"
  },"3": {
    "doc": "1. What You Are Reading",
    "title": "1. What You Are Reading",
    "content": "This is a civic design essay. It’s not a pitch, white paper, or research submission. It’s an exploration—part concept proposal, part system sketch, part cultural provocation. The core idea is technical: a lightweight, modular tool to highlight flawed reasoning in digital conversation. But the surrounding context is human. Tone, trust, and interpretation matter as much as model performance. So this paper moves between domains—language models, argument theory, pedagogy, and UX—and switches tone often. Some sections are nerdy. Others are more playful, skeptical, or intentionally jarring. If that feels uneven, it’s on purpose. Structural humility isn’t just a design constraint—it’s a writing constraint, too. This is for builders, educators, interface designers, and critics. You don’t have to agree. But if it makes you pause, test, or reframe, then it’s doing its job. Previous Next . ",
    "url": "/fallacytag/pages/01-what-you-are-reading/",
    
    "relUrl": "/pages/01-what-you-are-reading/"
  },"4": {
    "doc": "2. The Architecture of Avoidance",
    "title": "2. The Architecture of Avoidance",
    "content": " ",
    "url": "/fallacytag/pages/02-the-architecture-of-avoidance/",
    
    "relUrl": "/pages/02-the-architecture-of-avoidance/"
  },"5": {
    "doc": "2. The Architecture of Avoidance",
    "title": "Table of Contents",
    "content": ". | 2.0 What’s Avoided at Scale | 2.1. The Decline Question | 2.2 A Fork in the Feedback | . 2.0 What’s Avoided at Scale . Poet Andrew Choate holds up a mirror at an angle that flatters few: . Is it lazier to make a big thing, make a lot of things, or to make a small thing, not very much? There’s certainly an aura—perhaps even a suspicion—of laziness in prolificness. Choate’s insight is more than artistic musing—it touches something structural. In this framing, laziness isn’t about the amount of visible effort—how prolific or energetic someone appears—but about the effort that’s systematically avoided: the harder, slower work of cognitive depth—like tracking internal consistency across ideas, integrating perspectives, or wrestling with moral consequences. A person who constantly produces is not thinking hard enough. Reasoning is expensive biologically1: it draws heavily on focus, memory, and effortful synthesis. What’s avoided depends on the person and context—attention span, cognitive bandwidth, platform norms, and even educational background. People think just well enough to get by most of the time, which is to say, we don’t think well often. In the digital realm, the collective result of these savings is hard to miss even for a budget observer. The concept of productive laziness scales perfectly to the Internet, where prolificness dominates—across users who favor immediacy over depth, across platforms that amplify volume over structure, and across time as habits solidify into norms. We churn out tweets, comments, videos, and threads—creative, at times, but rarely cumulative. They seldom connect, challenge, or extend ideas in ways that build reasoning2. Entranced in sweaty pursuit of algorithmic incentives and social validation, we don’t resist the omission. Because reasoning is costly and platforms are optimized for metabolic thrift (privileging ease, speed, and volume), deeper forms of thought get displaced. Individual shortcuts are just that, but how the system responds to them invites scrutiny. The more we reward rapid, prolific contributions, the more those patterns dominate—and the more algorithms tune themselves to those patterns in turn. Poor reasoning isn’t just common; it’s reinforced. A positive feedback loop emerges: lazy thinking gets surfaced, incentivized, copied, and rewarded, until it becomes the baseline rhythm of public discourse. What’s avoided at scale becomes not just a personal lapse, but a structural erosion—a thinning of what holds thought together. Like a microphone too close to a speaker, the system begins to amplify its distortion. flowchart TD U[User Input: Quick, Shallow Content] P[Platform Incentives: Likes, Shares, Visibility] A[Algorithmic Prioritization: Surface &amp; Amplify] N[Norm Formation: Prolificness = Success] B[User Behavior Shifts: More Shortcut Thinking] U --&gt; P --&gt; A --&gt; N --&gt; B --&gt; U classDef loop fill:#fffbe6,stroke:#c9a100,stroke-width:2px; class U,P,A,N,B loop . Figure: The Reasoning Feedback Loop. And so, the Internet has grown vast and expressive—yet hollowed of connective logic3. This didn’t happen by decree—it emerged through a kind of ambient laziness: the positive feedback loop of systematic deferral of slow, effortful thought. 2.1. The Decline Question . We are taking a stroll among big ideas and dramatic questions, and here it’s paramount to walk past the wrong ones. I am reminded of a story that the legendary composer John Cage told in Indeterminacy: . After the concert I was walking along with the composer and he was telling me how the performances had not been quite up to snuff. So I said, “Well, I enjoyed the music, but I didn’t agree with that program note about there being too much pain in the world. He said, “What? Don’t you think there’s enough?” I said, “I think there’s just the right amount”. Cage’s provocation points out that saying “too much suffering” begs the question: relative to what ideal, what baseline, and by what mechanism are we measuring? This is precisely the kind of ambiguity that should give us pause before declaring reasoning in decline, whether due to platforms or anything else. Many critiques of modern discourse, from social media to political polarization, carry a background hum of nostalgia: the sense that people used to think better. But as a diagnostic, it’s deeply fraught. Measuring “reasoning” is hard enough. Comparing it across platforms, eras, and populations is harder still. What’s the unit of analysis? A syllogism? A thread? A voter base? A classroom? Should we weigh visibility, frequency, coherence, and influence? There’s no natural denominator—and no stable point of reference. Worse, the question may not even be consequential. Suppose we knew that the average quality of public reasoning had declined. What exactly would follow? Our task would still be to improve it. Or suppose reasoning had increased, but become less discoverable amid digital noise. That, too, would call for new tools and better infrastructure. We are looking to side-step the irony of dedicating the previous chapter to the sorry state of the online discourse, and avoid the descent into helpless lament. Rather than ask whether reasoning has declined, we prefer to ask: how can we support more of it, in more places, with less friction and more uptake? . We’re dissatisfied with the current reasoning environment, but we also see in it a challenge that lends itself, at least in part, to architectural solutions. 2.2 A Fork in the Feedback . The rise of large language models adds a new layer of complexity to the landscape we’ve just described. LLMs are trained on the same public discourse that platforms already distort—language shaped by speed, ambiguity, persuasion, and prolificness. So it’s reasonable to worry that they might amplify the positive feedback loop we’ve traced: reproducing shallow reasoning, reinforcing its patterns, and accelerating its spread4. But amplification is not an inevitability. While LLMs inherit the ambiguity and rhetorical shortcuts of human language, their behavior is shaped as much by architecture as by data. Prompt structures, interaction design, system incentives, and delivery context all influence what kind of reasoning they surface or suppress. In the wrong configuration, they amplify fallacies. But in the right one, they could help users notice what’s missing, slow down when it matters, and build with more structure. The challenge isn’t to perfect reasoning, but to scaffold it—to make its presence more visible, its absence harder to miss, and its structure easier to sustain. FallacyTag is one such attempt—but it’s not offered as proof of concept for the broader thesis. If it fails to change behavior or improve discourse, that outcome wouldn’t undermine the core claim: that reasoning can be shaped architecturally5. It might only mean this particular design—or this moment—wasn’t the right match. What matters is the direction of inquiry: building systems that make reasoning easier to see, to question, and to improve. If we can design for attention, we’d better design for reasoning. Previous Next . | Deliberate reasoning draws heavily on attention, memory, and executive control. Psychologists describe it as System 2 thinking—slow and effortful, unlike our fast, intuitive defaults (Stanovich &amp; West 2000; Kahneman 2011). Neuroscience shows it activates resource-intensive brain areas like the prefrontal cortex (Baddeley 2000), while cognitive load theory explains why unfamiliar reasoning tasks quickly overwhelm us (Sweller 1988). Evolutionary accounts argue that reasoning is selectively deployed—used when the stakes justify the energy cost, especially in social argument (Mercier &amp; Sperber, 2011; Lieder &amp; Griffiths 2020). Early studies have even tied sustained reasoning to glucose depletion (Gailliot et al. 2007) or, more recently, subjective fatigue (Kurzban et al. 2013). &#8617; . | Online discourse is often expressive or creative but rarely builds cumulative reasoning—a claim supported by multiple empirical studies. On Facebook, Del Vicario et al. (2016) showed that users interact primarily within ideological echo chambers, where emotionally resonant content spreads more effectively than deliberative argument. Singer et al. (2016) indicate diminished reasoning quality over time on Reddit, suggesting that platform interaction fails to reinforce sustained, cumulative reasoning. Aragón et al. (2017) note that drifting threads and ignored comments result from poor structuration of argument flow. Shortall et al. (2021) observe that digital platforms often degrade discussion quality when attempting to scale deliberation and highlight the scarcity of design features that support argument accumulation, critical thinking, and structured reasoning. Collectively, these studies suggest that while digital platforms facilitate creative participation, they seldom reward or reinforce the kind of reasoning that accumulates across exchanges. &#8617; . | We are aware of pockets of better reasoning on the Internet: longform forums, commentaries, Substacks, or collaborative writing spaces. The focus of this section is on what gets surfaced and rewarded at scale. The next section (2.1) explains that we are not interested in the authoritative grading of the Internet. &#8617; . | Large language models may amplify fallacious reasoning if not carefully constrained. Studies have found that while LLMs can identify certain fallacies, they often misclassify, overlook, or even generate flawed arguments, particularly in nuanced or ambiguous cases. For example, Teo et al. (2025) reported high variability in LLM fallacy classification performance, especially with intent-dependent types such as intentional or equivocation. Similarly, Manickavasagam &amp; Bandara (2025) noted that even with prompting and fine-tuning, models struggle with interpretive ambiguity. External evaluations have shown that LLMs frequently generate persuasive but fallacious content in argument tasks unless guided by structured reasoning constraints (Ji et al., 2023). &#8617; . | Several platforms exemplify how architectural design can scaffold cumulative reasoning online. Kialo structures debates via branching pro/con argument maps, which multiple (self-promoted but seemingly independent) educational studies and documentation show improve critical thinking, clarity, and engagement. Pol.is uses clustering algorithms to surface cross-cutting consensus, and its deployment in Taiwan’s vTaiwan platform has led to real policy outcomes (vTaiwan case study; The Computational Democracy Project case studies). LiquidFeedback supports structured proposition development, integrated deliberation and voting, and delegative democracy mechanisms used by the German Pirate Party (Kling et al., (2015)) and other civic groups (Behrens et al., 2014). These platforms illustrate that, with intentional architectural design, online reasoning can indeed accumulate meaningfully across exchanges. &#8617; . | . ",
    "url": "/fallacytag/pages/02-the-architecture-of-avoidance/#table-of-contents",
    
    "relUrl": "/pages/02-the-architecture-of-avoidance/#table-of-contents"
  },"6": {
    "doc": "3. Scaffolded Intelligence: How LLMs Can Support Better Reasoning",
    "title": "3. Scaffolded Intelligence: How LLMs Can Support Better Reasoning",
    "content": " ",
    "url": "/fallacytag/pages/03-scaffolded-intelligence/",
    
    "relUrl": "/pages/03-scaffolded-intelligence/"
  },"7": {
    "doc": "3. Scaffolded Intelligence: How LLMs Can Support Better Reasoning",
    "title": "Table of Contents",
    "content": ". | 3.0 A Glimpse of What’s Coming | 3.1 What Can Work Now: A Narrower but Useful Path | 3.2 Designing Within Limits | 3.3 Nudges as Pedagogical Design | . Can large language models identify fallacies? Not reliably. However, we can set conditions that allow us to capture much of the potential value. We begin with an anecdote that showcases an LLM’s surprising moment of clarity (Section 3.0), and probe the conditions under which that clarity arises (3.1). From there, we sketch a design path that works within those limits rather than against them (3.2), and argue that the resulting interaction model may not only be sufficient, but also desirable (3.3). 3.0 A Glimpse of What’s Coming . Not long ago, I watched a well-intentioned, skillfully produced 23-minute YouTube video with my son. It explored Arrow’s Impossibility Theorem and the limitations of ranked voting systems. The analysis was methodical — until the final minute, when the tone subtly shifted from formal analysis to civic appeal. Suddenly, “democracy” was no longer a voting procedure but a cherished ideal. That Churchill quote appeared — never a good sign1. An emotionally satisfying, logically unmoored closing followed. My antenna twitched hard enough to unseat my trucker hat. Mildly bothered but unwilling to allocate much CPU to parenting, I brushed off the moment with a few glib remarks and let GPT-4 do the explaining. I asked the reportedly shaky logician to assess the reasoning in the video’s conclusion. Without coaching or fine-tuning, the bot accurately autocompleted some fallacies that are generally thorny to detect — such as equivocation, appeal to sentiment, and category error — far beyond what I expected. This wasn’t rote fallacy labeling. The model demonstrated contextual sensitivity — tracking meaning shifts, identifying rhetorical substitutions for logic, and anchoring its analysis in the argument’s actual structure. Teo et al. (2025) found that LLMs stumble on these flaws. They tested multiple LLMs on a structured fallacy classification task and found that performance was strongest for shallow, syntactically cued fallacies such as ad hominem or circular reasoning. Success rates dropped sharply when fallacies demanded contextual reasoning — tracking shifts, ambiguity, or intent. In those cases, even large models struggled to apply definitions or reason consistently across contexts. The pattern suggests that fallacy detection isn’t just a matter of fluency, but of structural grounding — something today’s models rarely do reliably unless the structure is already implicit in the input. And then it hit me: I had scaffolded the task. Not intentionally, I was just talking through what I wanted explained to my kid, but the result looked suspiciously like prompt engineering. Great. I’d become part of the model’s reasoning pipeline. The surprise wasn’t that the model flagged obvious errors — but that it caught reasoning drift you’d expect it to miss. The model identified those subtler flaws accurately, not because it understood, but because I shaped the task in a way that allowed it to navigate2. A fluent moment — but scaffolded. That’s what makes it interesting: it invites a reframing. Not “Can models detect fallacies?” — but “What if we designed systems around reasoning structure, not correctness?” . That’s the hinge: fallacy detection is brittle — brittle to tone, intent, ambiguity. But surfacing structure? That’s tractable. And if done well, transformative. 3.1 What Can Work Now: A Narrower but Useful Path . If the model needed me in the loop to think straight, at least one of us was in trouble. So I tried a simpler question: . Can a general-purpose LLM, with a plain prompt and no scaffolding, surface informal fallacies in a way that feels grounded and legible to a layperson? . Not reliably, not exhaustively — just, sometimes, usefully. I ran a comically small experiment to test it. I went to Reddit’s r/ChangeMyView, picked the first thread that wasn’t a political minefield, and pulled three consecutive recent comments. I fed them, one by one, into GPT-4 with a basic prompt: identify any flawed reasoning and explain why. No special formatting, no chain-of-thought nudges. The thread’s claim was typical: “Local politics is much more important to your life than national politics.” The replies were sincere but messy—ripe for overreach. | One cited Canadian municipalities to argue national politics mattered more. GPT flagged a false analogy and unsupported empirical claim. Fair. | Another drew a neat line: local = speed bumps, national = healthcare. GPT pointed to false equivalence and oversimplification. Again, fair. | A third listed federal programs to argue that national politics is more consequential. GPT noted cherry-picking and scope shifting—raising lifetime-scale policies to counter local day-to-day effects. | . This wasn’t exactly a rigorous evaluation — three comments, one researcher, zero inter-rater reliability. However, under plain conditions, the model provided structured and sensible feedback. This is where prior studies suggest models have the best chance of success — and where a product like FallacyTag might be viable. 3.2 Designing Within Limits . FallacyTag isn’t a truth judge. It flags form, not verdicts. Instead of asking whether a claim is right or wrong, it highlights how the reasoning is shaped—inviting reflection rather than imposing judgment. That shift in design intent makes all the difference. Structure-surfacing is tractable because it sidesteps the hardest parts of reasoning. Here’s how that tractability shifts depending on the type of reasoning and the structure of the input: . |   | Plain but Coherent Text | Disjoined Text | . | Surface-Level Fallacies | ✅ Reliable | ⚠️ Sometimes (depends on clarity) | . | Contextual Fallacies | ⚠️ Possible (with scaffolding) | ❌ Fails: too messy / brittle | . GPT-4 can help—but, by itself, it’s not enough of an answer. The earlier examples worked because the task was shaped to fit the model’s strengths: clear text, stable discourse, no ambiguity about roles. The model followed a structure I didn’t even realize I was providing. That’s not product behavior—it’s scaffolded response. Why doesn’t that generalize? Because the hard parts remain hard. LLMs still flounder with messy discourse, shifting tone, unstable goals. They don’t know when they’re helping—especially when reasoning becomes social, not just logical. A correct tag, surfaced at the wrong moment, can derail reflection. Reasoning isn’t just about logic. It’s about timing, tone, and trust. That’s not about accuracy. It’s about fit—the difference between a system that clarifies and one that corrodes. Prompts can’t patch that gap. Prior efforts have tried: instruction tuning, custom prompting, multi-turn clarification34. These work in ideal settings, with motivated users.567 They don’t scale to noisy, asynchronous discourse. FallacyTag takes a different tack: it accepts the limit and designs within it. No general truth judgments. No deep inferences. Just structural cues surfaced where the model’s fluency is stable—and offered only when useful. So the MVP has to respect both constraints: . | Model limits define what can be flagged. | Interface design defines how—and whether—it should be shown. | . That path is narrow but viable: sentence-level cues, tuned to form over content, optional and deferential. FallacyTag doesn’t try to be right—it tries to help. It surfaces the shape of reasoning: binary frames, abrupt pivots, unstable structure. Sometimes that shape hints at a fallacy; sometimes it just invites a second look. This isn’t correctness automation. It’s structural affordance. And that’s the bet: that even a soft, nonjudgmental tag — if offered at the right time, with the right tone — can open space for reflection. 3.3 Nudges as Pedagogical Design . Leo Tolstoy spent years trying to define art, writing a dense philosophical tome that sprawls across almost 200 pages—only to arrive at a surprisingly simple idea: art is the transmission of emotion from artist to audience. He could have said it in a few paragraphs. After reading this section, you may be able to relate to Tolstoy’s readers. The interaction model outlined in Section 3.2—where the system highlights reasoning that may warrant another look without outright calling it fallacious—is not just a fallback when the model is uncertain. It may be the best way to promote critical thinking, precisely because of how it distributes both cognitive and social responsibility. What we discovered on our trek through the Himalayas, driven by technical necessity, was always nearby, on level ground, with signs, if we looked through the lens of pedagogical virtue. From a learning science perspective, brief cognitive interruptions that demand user engagement are often more effective than immediate explanations. Theories of desirable difficulties8 show that introducing effortful, nontrivial steps—such as prompting a learner to retrieve or self-explain—can improve both retention and transfer. Rather than simply delivering the “correct” fallacy label, a nudge invites the user to reflect: Could something be wrong here? Why might this feel off? This preserves the productive discomfort that is central to metacognitive learning9, aligning with research that shows that prompting reflection improves the quality of argumentative writing more than providing corrective feedback alone10. Socially, nudges avoid the defensive posture often triggered by direct correction. In domains where argument intersects with identity—such as politics, culture, or values—overt labeling of fallacies can be perceived as dismissive or adversarial. Behavioral research on reactance11 suggests that people are more likely to resist information when it threatens autonomy. Soft interventions such as prompts or open-ended cues tend to preserve engagement, especially in mixed-audience or ideologically diverse environments. A system that says, “You may want to rethink this” invites participation; one that says, “You’re wrong” invites rejection. Figure: The surprising power of a soft touch. FallacyTag’s “nudge” design isn’t a fallback. It’s a convergence point: four fields of research point to the same principle—people think better when they’re invited to reflect, not told they’re wrong. This approach also scales better across uncertain or ambiguous cases. In many instances, what appears to be a fallacy may hinge on context, unstated assumptions, or interpretive nuance. Flagging such moments as “potentially problematic” rather than conclusively fallacious reflects appropriate epistemic humility. It helps the system avoid overclaiming and keeps the user informed, encouraging co-interpretation rather than authority-based override. Finally, this model has the potential to shift norms12. A system that reliably surfaces “moments worth pausing over” could, at scale, foster new habits of attentiveness. Rather than modeling omniscient critique, it models curiosity. And rather than framing fallacy detection as adversarial or corrective, it reframes it as an act of intellectual care: paying attention to structure not to win, but to gain a deeper understanding. In this way, what began as a workaround for model fallibility becomes a design stance toward better reasoning—both human and machine. Designing for fallacy detection under uncertainty might seem like a compromise. But as the shape of the solution emerges—from selective signaling to user-side engagement—it begins to look less like a concession and more like a blueprint for something better: a reasoning aid that doesn’t dictate truth, but invites rigor. One that foregrounds structure without overreach—a useful pattern not just for machines, but for people. Next, we’ll explore what it means to build from those boundaries. Previous Next . | Antifreeze &amp; Coolant is the worst shampoo — except for all the others in my garage. No, we shouldn’t submit to Churchill’s appeal to relative privation and stop seeking better forms of government. &#8617; . | The source material helped: a serious, methodical, good-faith analysis with a single jarring tonal shift. In benchmark terms, this was a clean input — not a chaotic social media thread. The conditions were ideal for the model’s strengths. &#8617; . | Du, K., Xing, F., Mao, R., &amp; Cambria, E. (2024). The authors demonstrate that GPT-3.5 and PaLM-2 lack robust reasoning ability even in a structured domain like finance, particularly in tasks involving comparative and numerical reasoning. Performance improved significantly only when tasks were scaffolded with attribute-specific prompts—suggesting that without external structure, reasoning degrades even under domain constraints. &#8617; . | Lim, G., &amp; Perrault, S. (2024). While GPT-4 achieved high accuracy on a curated fallacy classification task, success required prompt tuning and failed to generalize to subtler or interpretive fallacies. The authors caution against over-interpreting such results as evidence of deep reasoning capacity, especially for real-world applications. &#8617; . | Manickavasagam, A., &amp; Bandara, D. (2025). Logical fallacy detection performance improves with zero-shot chain-of-thought prompting and task-specific fine-tuning; however, success is highly sensitive to the prompt format and context. Even with a tuned GPT-3.5 model, accuracy varied across fallacy types, with interpretive and intent-based categories remaining brittle. &#8617; . | Alhindi, T., Chakrabarty, T., Musi, E., &amp; Muresan, S. (2023). Their method uses multitask prompts and dataset-aware formatting to improve generalization across structured fallacy corpora. However, success still depends on having clean data and ideal conditions—fallacies are labeled ahead of time, prompts are structured, and ambiguity is minimized. &#8617; . | Schneider, J., Haag, S., &amp; Kruse, L. (2025). In a negotiation game with GPT-3.5, the authors demonstrate that users can “hack” the model through strategic misdirection, repetition, or invented flaws—highlighting not only reasoning vulnerabilities but also the inaccessibility of prompt engineering to most participants. The wide variance in outcomes exposes serious gaps in AI literacy and reliability. &#8617; . | Bjork, R. A., &amp; Bjork, E. L. (2011). Making things hard on yourself, but in a good way: Creating desirable difficulties to enhance learning. In D. S. Dunn (Ed.), Psychology and the Real World: Essays Illustrating Fundamental Contributions to Society (pp. 56–64). Worth Publishers. &#8617; . | Schraw, G., &amp; Dennison, R. S. (1994). This paper introduces a widely used framework for assessing metacognitive regulation. The distinction between cognition and metacognition is helpful here: FallacyTag doesn’t merely flag content errors—it prompts awareness of reasoning itself, nudging users toward reflection about how they think, not just what they say. &#8617; . | Nussbaum, E. M., &amp; Kardash, C. M. (2005). In this study, students who were prompted to reflect or generate counterarguments produced more sophisticated arguments than those who received direct corrective feedback. It supports the idea that gentle prompts to reconsider structure may be more educationally valuable than simply identifying a fallacy label. &#8617; . | Brehm, S. S., &amp; Brehm, J. W. (1981). Psychological Reactance: A Theory of Freedom and Control. Academic Press. &#8617; . | See 2. The Architecture of Avoidance, where we traced a positive feedback loop that erodes public reasoning by making structure harder to see and therefore less likely to be used. If FallacyTag can surface structure gently, even sporadically, it might dampen that loop. That’s not just a UI choice—it’s a norm intervention. &#8617; . | . ",
    "url": "/fallacytag/pages/03-scaffolded-intelligence/#table-of-contents",
    
    "relUrl": "/pages/03-scaffolded-intelligence/#table-of-contents"
  },"8": {
    "doc": "4. FallacyTag: A Gentle Loop for Sharper Thinking",
    "title": "4. FallacyTag: A Gentle Loop for Sharper Thinking",
    "content": " ",
    "url": "/fallacytag/pages/04-introducing-fallacytag/",
    
    "relUrl": "/pages/04-introducing-fallacytag/"
  },"9": {
    "doc": "4. FallacyTag: A Gentle Loop for Sharper Thinking",
    "title": "Table of Contents",
    "content": ". | 4.1 Demo: What You’re Seeing | 4.2 Tagging Loop | 4.3 Related Work | . FallacyTag is a lightweight reasoning aid—a subtle seismograph for argument structure. It doesn’t declare truth or call out errors. It just picks up on small tremors in reasoning—moments where things might not follow—and sends a gentle signal your way. The nudge is quiet, contextual, and optional. You’re free to ignore it. Or to pause, look closer, and decide for yourself. 4.1 Demo: What You’re Seeing . Your browser does not support the video tag. 👉 Try it yourself: coherentdrift.github.io/fallacytag/demo . The demo reveals more than an interface. The tagged text isn’t synthetic—it was generated using the prompt strategy implied in the previous section, applied to a real published opinion article1. What you’re seeing is not just a surface mockup, but an early sketch of what FallacyTag can become: a reasoning-aware layer that pairs content, critique, and context. 4.2 Tagging Loop . The core of FallacyTag can be traced in a single arc: . | A piece of content is analyzed | A reasoning flaw is detected | A tag is gently surfaced | . Then the loop pauses—for the user. This process is visualized below: . Figure: The FallacyTag interaction loop. Tags are rendered on reasoning flaws, and users may dismiss, revise, or ignore them—each action feeding back differently into the system. The system doesn’t push. It marks a moment of uncertainty—and steps back. From there, it depends on whether you’re reading or writing—and the kind of attention you’re bringing to the page. Some readers will nod, mentally flag the moment, and move on. Others may tap the tag to expand it—or mark it as incorrect. In some interfaces, dismissing a tag may even remove the highlight—quietly affirming the user’s decision and restoring flow. And some—if they’re in writing mode—may return to the source and revise it. Each path keeps agency in the user’s hands. The tag never insists. FallacyTag allows users to mark a tag as helpful or incorrect, but whether that feedback can meaningfully improve the system is an open question. 4.3 Related Work . We don’t know how to build better models, but we hope to make better readers. FallacyTag draws on recent research on how large language models (LLMs) can detect logical fallacies in natural language. Recent studies have fine-tuned models like GPT-3.5 and evaluated open-source LLMs (e.g., LLaMA, Phi, Gemma) on benchmark datasets, with techniques like zero-shot chain-of-thought prompting significantly boosting classification performance2. However, the academic work generally treats fallacy detection as a self-contained NLP task, focusing on accurately classifying errors. FallacyTag has a different approach. It considers model outputs not as definitive labels but as interpretive cues—presented in context, embedded in text, and always optional. The goal is not to perfect detection3 but to promote perception: to help users recognize when reasoning may not hold. This reframing places FallacyTag closer to a pragmatic tradition of argument analysis. Rather than policing correctness, it supports reflection and dialogue—echoing theories that view fallacies as context-sensitive missteps rather than fixed logical violations4. FallacyTag also works in messier terrain. While many benchmarks focus on short, synthetic examples, FallacyTag engages with real-world discourse, including essays, transcripts, and commentary. Its purpose isn’t to declare “wrong.” It’s to gently ask: “Might this not follow?” . FallacyTag operates on transcripts—textual representations of speech, video, or writing—enabling it to work across various formats. But the way it lands depends on the medium. That’s the challenge we take up next. Previous Next . | The full prompt is available inside the demo. &#8617; . | See Teo et al. (2025) and Manickavasagam &amp; Bandara (2025) for overviews of LLM-based fallacy detection. These studies fine-tuned models like GPT-3.5 on datasets such as LOGIC and LOGIC Climate, achieving F1 scores above 0.8 in some configurations using chain-of-thought prompting and few-shot learning. &#8617; . | To be clear: we are not objecting to others perfecting detection - it’s a valuable goal, and it’s clear why researchers focus on it. We do not contribute to that effort in this paper. &#8617; . | This perspective is drawn from the work of Douglas Walton and Frans van Eemeren, who argue that fallacies are better understood as context-dependent breakdowns in dialogue rather than as fixed, formal violations of reasoning. See also historical discussions in Hamblin (1970) as well as later developments in computational argumentation theory (e.g., argumentation frameworks, dialogue games, and defeasible reasoning systems treat fallacies as strategic moves gone wrong or contextually inappropriate actions.) &#8617; . | . ",
    "url": "/fallacytag/pages/04-introducing-fallacytag/#table-of-contents",
    
    "relUrl": "/pages/04-introducing-fallacytag/#table-of-contents"
  },"10": {
    "doc": "5. Mediums, Modalities and Fit  ",
    "title": "5. Mediums, Modalities and Fit",
    "content": "Designing FallacyTag for Text, Audio, and Video . ",
    "url": "/fallacytag/pages/05-modalities-and-fit/#5-mediums-modalities-and-fit",
    
    "relUrl": "/pages/05-modalities-and-fit/#5-mediums-modalities-and-fit"
  },"11": {
    "doc": "5. Mediums, Modalities and Fit  ",
    "title": "Table of Contents",
    "content": ". | 5. Mediums, Modalities and Fit . | 5.1 Text: Where Reasoning Is Most Visible . | 5.1.1 Adapter Implications (Technical Layer) | 5.1.2 Observed Modalities (Interaction Layer) | 5.1.3 Fit and Design Implications | . | 5.2 Audio: Designed for Flow, Not Friction . | 5.2.1 Adapter Implications (Technical Layer) | 5.2.2 Observed Modalities (Interaction Layer) | 5.2.3 Fit and Design Implications | . | 5.3 Video: The Tightrope Between Pacing and Pause . | 5.3.1 Adapter Implications (Technical Layer) | 5.3.2 Observed Modalities (Interaction Layer) | . | 5.3.3 Fit and Design Implications | 5.4 Trade-Offs Across the Flow–Friction Spectrum | 5.5 From Logic to Delivery: Adapting the Pipeline | . | . FallacyTag is built around language. But language unfolds differently depending on where—and how—we encounter it. A Reddit debate, a scripted podcast, and a live Twitch stream all rely on words, but differ radically in pace, tone, and user intent. These differences aren’t cosmetic. They shape how reasoning is presented, perceived, and whether it can be usefully tagged. Two dimensions matter. First, the medium—text, audio, video—shapes the technical surface where tagging can occur. Adapters are medium-specific: a browser DOM, a podcast transcript, a video caption stream. This is the entry point for implementation. But equally important is the modality: the interaction pattern through which language unfolds. A Reddit thread, a podcast monologue, and a live Twitch stream are all built on language, but they differ in tone, timing, pace, and user expectation. These elements govern whether the reasoning structure is present, visible, and taggable. In short, medium shapes delivery; modality shapes fit. Figure: Taxonomy of reasoning contexts by medium and modality . We walk through each medium—not because it’s the dominant factor, but because each hosts a characteristic range of modalities with recurring challenges and affordances. We close with a synthesis of how these patterns inform adapter design and feedback orchestration. This discussion also fills a gap in prior work. While most fallacy detection research emphasizes classification accuracy and model performance, relatively little attention has been paid to how reasoning feedback should be delivered across modalities. To our knowledge, no existing system addresses this challenge directly. We aim to surface it here. We walk through each medium—not because it’s the dominant factor, but because each hosts a characteristic range of modalities with recurring challenges and affordances. We close with a synthesis of how these patterns inform adapter design and feedback orchestration. This discussion also fills a gap in prior work. While most fallacy detection research emphasizes classification accuracy and model performance, relatively little attention has been paid to how reasoning feedback should be delivered across modalities. To our knowledge, no existing system addresses this challenge directly. We aim to surface it here. 5.1 Text: Where Reasoning Is Most Visible . 5.1.1 Adapter Implications (Technical Layer) . Text is where FallacyTag is most natural to implement. The medium exposes a well-structured surface—DOMs, Markdown documents, transcripts—onto which tags can be injected with minimal friction. Parsing is straightforward. Delivery can be inline or layered, synchronous or deferred. In many cases, existing user-facing interfaces (e.g., browsers, e-readers, collaborative editors) can be augmented via extensions or overlays, without requiring platform integration. This makes text a clear first target for adapters. 5.1.2 Observed Modalities (Interaction Layer) . But “text” encompasses a wide range of modalities, shaped by platform dynamics, pacing, and user goals. Examples include: . | Deliberative (Reddit structured debate threads, peer feedback on learning Platforms like Khan Academy or Coursera, where learners receive logic-aware reviews) | Persuasive (Substack or Medium blog posts, which mix persuasion and commentary, often with explicit argument structure) | Informal (comment threads, general web reading - from essays to corporate blogs) | . Despite variability in tone, many of these environments expose reasoning in ways that make constructive surfacing possible. 5.1.3 Fit and Design Implications . FallacyTag works best when reasoning is visible, valued, and revisable. This includes: . | Longform argumentation (e.g., newsletters, essays) | Structured feedback and review (e.g., peer critique systems) | Reflective educational contexts (e.g., student forums or tutoring platforms) | . In these cases, tagging can appear inline with minimal disruption. Users can hover, tap, or click for explanations, and respond with lightweight feedback (“thanks,” “incorrect,” etc.) These modalities support both reader-facing workflows (surfacing reasoning in content consumption) and author-facing workflows (offering logic-aware cues during drafting or revision). This dual role—supporting both readers and writers—makes text-based environments an especially compelling fit for FallacyTag. In some structured environments—particularly in education—FallacyTag may even support manual tagging or confirmation by educators, reviewers, or peer authors. These hybrid human+AI workflows are not the primary design path, but they open up new roles for reasoning-aware authoring support. By contrast, modalities like adversarial comment threads or outrage-driven forums are low-fit environments. Even well-calibrated tags risk being ignored or reinterpreted as aggression, irony, or partisanship. In such spaces, tags should be optional, passive, or private by default, and feedback mechanisms should account for tone volatility. 5.2 Audio: Designed for Flow, Not Friction . 5.2.1 Adapter Implications (Technical Layer) . Audio presents a more constrained surface for tagging. There is no native text layer; tags must anchor to transcripts, timestamps, or linked metadata. Adapters must handle: . | Transcript alignment: ensuring tags map to precise audio segments | Trigger design: determining how tags are accessed—e.g., via playback-linked cues, transcript overlays, or post-hoc bundles | Playback integration: embedding tags without disrupting listening flow | . Some platforms (e.g., Overcast, Spotify) support enriched metadata or interactive transcripts; others do not. Delivery remains contingent on third-party APIs or browser-based augmentation. The core UX challenge is this: FallacyTag is designed as a nudge—a lightweight prompt to pause and think. But most audio platforms are built around flow. Listeners may be jogging, driving, or multitasking. Their goal is sustained engagement, not interruption. What is supposed to happen when a tag appears? Should the listener stop? Remember the moment? Open their phone? These conditions make not just technical delivery harder, but reflection itself less viable—especially in the moment. 5.2.2 Observed Modalities (Interaction Layer) . | Audio modalities tend to fall into three categories: | Scripted monologues: podcasts, essays read aloud, and commentary | Conversational formats: interviews, live chats, or recorded panels | Ambient streams: background listening where attention is partial | . Spoken formats carry arguments wrapped in rhythm, tone, and persona—often more performance than premise-evidence-conclusion. Listeners “feel” the arc without always registering its logical scaffolding. Unlike text, you can’t scan back to study how pieces fit. Fallacies are harder to notice in speech because the format mutes structure. Still, scripted podcasts—especially those with educational, analytic, or editorial aims—offer an opening. Transcripts exist. Reasoning is present. User interest is aligned. 5.2.3 Fit and Design Implications . FallacyTag is most viable in scripted or semi-structured audio, where reasoning is clear and transcripts are available. In these contexts, tags can be surfaced via interactive transcripts, playback-linked cues (e.g., “🧠 at 18:32”), or bundled into post-listening summaries. Each route depends on platform capabilities, but all share the constraint of limited user attention. Real-time interaction remains difficult. Audio is often consumed while multitasking, and feedback mechanisms like taps or swipes are rarely accessible in the moment. Even simple reactions (“helpful?”) may be ignored. In most cases, tagging must either be subtle and asynchronous or deferred entirely. In audio, the tradeoff between flow and friction becomes especially sharp. Here, the design challenge is to support reflection without disrupting flow. FallacyTag must act more like a whisper than a poke—a reminder stored for later, not a demand made now. On some platforms, transcript overlays may be sufficient. Elsewhere—especially in live or mobile-first audio—FallacyTag may need to shift from live nudges to post-listening review tools—a better fit for learners, researchers, or other serious listeners. 5.3 Video: The Tightrope Between Pacing and Pause . 5.3.1 Adapter Implications (Technical Layer) . Video combines the challenges of audio with the advantages of screen-based interaction. While users are watching—not reading—many platforms provide caption streams or machine-generated transcripts, giving FallacyTag a viable text layer for analysis. Adapters can anchor tags to timestamps or transcript segments and surface them via: . | On-screen overlays (e.g., floated annotations synced to playback) | Transcript enhancements (e.g., scroll-sync with tagged spans) | Post-video summaries (e.g., logic digests with jump links) | . Compared to audio-only formats, delivery is more tractable: the user is already in front of a screen, making taps, hovers, or pauses more reasonable. Still, the interaction surface is shared with dynamic visuals and often attention-grabbing content, making subtlety essential. 5.3.2 Observed Modalities (Interaction Layer) . Video modalities vary widely in tone, purpose, and pacing. Strong candidates include: . | Educational explainers (e.g., Veritasium, Kurzgesagt), where structured reasoning is scripted and visualized | Instructional platforms (e.g., Khan Academy, edX, Skillshare), where transcripts align tightly with lesson logic | Live or recorded meetings (e.g., Zoom with auto-captions), where tagging might aid review or live moderation | . Narrated product demos or pitches, mainly when persuasive framing drives the narrative . These modalities range from tightly produced to semi-improvised, but many share two useful traits: visible reasoning structure and a receptive audience mindset. 5.3.3 Fit and Design Implications . FallacyTag is best suited for scripted or semi-scripted videos, especially when transcripts or caption streams are aligned with the argument structure. Delivery options include: . | Timed overlays, which surface tags as the speaker reaches key lines | Enhanced transcript panes, with reasoning highlights linked to playback | Post-video summaries, offering logic maps or flagged claims with timestamps | . Real-time feedback remains limited; users are watching, not interacting. But light-touch affordances—e.g., dismissible overlays or hover-to-expand prompts—can enable interaction without disrupting flow. Depending on platform constraints, tags may be precomputed and anchored or generated dynamically from caption streams. Like audio, video demands flow—but unlike audio, it competes for attention with movement, cuts, and visual effects. The challenge is not just technical, but also cognitive: to surface logic cues in ways that feel timely yet non-intrusive, precise but light—that is, aligned with moments of cognitive slack. Audio resists interruption. Text invites it. Video sits between them, demanding careful threading of signal and subtlety. Figure: Flow vs. Friction Spectrum . 5.4 Trade-Offs Across the Flow–Friction Spectrum . Each of these modalities presents different trade-offs, but the goal remains the same: to surface flawed logic gently. Some environments make that easier than others. Here’s how they compare: . | Modality | Fit | User Effort | Rationale | . | Text | Best | Low | Users are already in a reading context; inline tags feel natural; arguments tend to be structured | . | Video | Good | Medium | Screen-focused consumption makes pause-and-read viable; many videos are tightly scripted | . | Audio | Weak | High | Users are engaged in flow activities; real-time logic nudges may break attentional rhythm | . That’s why our initial focus is on browser-based reading, structured peer feedback, and lightly augmented video playback. These modalities offer the best trade-off between cognitive fit and delivery feasibility. Users are already primed to engage with reasoning, and the technical surfaces support timely, low-friction interventions. In cases where multiple representations exist (e.g., video + transcript), FallacyTag aims to provide consistent tagging across views, preserving reasoning cues even as interaction modes shift. The delivery format may influence the style or granularity of the explanation, but the underlying structure remains stable. These starting points also offer a scalable base for broader deployment. Browser overlays and transcript-based adapters require no deep platform integration and can reach users across many sites and tools. By starting where both infrastructure and attention are available, FallacyTag can refine its methods before tackling harder environments, such as real-time speech, mobile-first platforms, or visually dense media. 5.5 From Logic to Delivery: Adapting the Pipeline . FallacyTag uses the same core reasoning engine across all media, but the execution path varies by medium. Input format, UI surface, feedback channel, and timing model all shift depending on whether the content is read, heard, or watched. These differences aren’t just cosmetic—they affect what’s possible and what makes sense. The table below breaks this down. Text-based deployments allow for inline tagging with low friction and rich user feedback. Audio, by contrast, requires transcript anchoring and post hoc interaction. Video sits in the middle: it benefits from visual anchors and screen presence, but still contends with timing sensitivity and competing visual demands. | Aspect | Text-Based Reading | Audio-Only Listening | Audio-Visual (Video) | . | Input Format | Raw or DOM-parsed text (Reddit, Medium) | Auto-generated transcript (podcast, talk) | Captions/transcript (YouTube, Zoom) | . | Preprocessing | Inline span segmentation, quote filters | Timestamped text extraction, speaker cues | Caption alignment, sentence stitching | . | UI Integration | Inline overlays, hover or click to expand | Transcript summary, timestamped tag index | Scroll-sync overlays, caption-layer tags | . | Timing Model | Real-time or on-load tagging | Post hoc only (non-interruptive) | Inline or scroll-synced, possibly deferred | . | Feedback | Direct interaction (dismiss, expand) | Optional post-listening rating or flag | Tap/click during playback or review mode | . | UX Constraints | Must not disrupt reading flow | Tags must avoid intrusion during listening | Must avoid screen clutter, timing friction | . The architecture diagram sketches how these medium-specific layers fit into the system as a whole: . Figure: How FallacyTag Adapts to Text, Audio, and Video Surfaces . While the core architecture treats all media—text, audio, and video—as inputs to a shared reasoning pipeline, the implementation diverges at the delivery layer. A Reddit comment and a YouTube transcript may flow through the same core inference engine, but the way tags are rendered, timed, and interacted with varies significantly. Browser-based overlays may appear inline during reading; podcast transcript tags might index timestamped summaries post hoc; video overlays might sync with caption lines during playback. These differences don’t change the core logic, but they do shape user experience, latency expectations, and interface behavior. Delivery orchestration is thus surface-specific, even when the underlying reasoning logic stays consistent. The divergence isn’t just architectural—it’s experiential. Each modality shapes how the user perceives the tag, when they can respond, and what kind of feedback is even possible. To visualize how this plays out in practice, the following figure walks through representative system flows for each medium. All begin with input parsing and reasoning detection. But from there, the user experience diverges: . | Text tags appear inline, with full interaction and feedback | Audio tags link to moments in time, with optional voice/button triggers | Video tags float visually, balancing timing, clarity, and minimal interruption | . Figure: How the tagging pipeline plays out differently for text, audio, and video. Though the reasoning logic is shared, the form of nudging is not. This is the core tension FallacyTag is built to navigate: logic wants to be surfaced, but attention doesn’t want to be broken. The tradeoff—flow versus friction—varies by medium. What works for a blog post may misfire in a podcast. FallacyTag’s logic layer stays constant, but its delivery adapts—always tuned to where users are and how they engage. FallacyTag’s delivery must adapt to each surface, but behind that flexibility lies a common core. Whether analyzing a Reddit post, a podcast transcript, or a captioned video, the same system interprets the text, identifies potential reasoning flaws, and returns structured tags. The variability lies in how those tags are surfaced and received. In the next section, we pull back the curtain on how this shared pipeline works, step by step. Previous Next . ",
    "url": "/fallacytag/pages/05-modalities-and-fit/#table-of-contents",
    
    "relUrl": "/pages/05-modalities-and-fit/#table-of-contents"
  },"12": {
    "doc": "5. Mediums, Modalities and Fit  ",
    "title": "5. Mediums, Modalities and Fit  ",
    "content": " ",
    "url": "/fallacytag/pages/05-modalities-and-fit/",
    
    "relUrl": "/pages/05-modalities-and-fit/"
  },"13": {
    "doc": "6. How It Works",
    "title": "6. How It Works",
    "content": "Coming soon… . ",
    "url": "/fallacytag/pages/06-how-it-works/",
    
    "relUrl": "/pages/06-how-it-works/"
  },"14": {
    "doc": "7. Is FallacyTag Feasible?",
    "title": "7. Is FallacyTag Feasible?",
    "content": "Coming soon… . ",
    "url": "/fallacytag/pages/07-feasibility/",
    
    "relUrl": "/pages/07-feasibility/"
  },"15": {
    "doc": "8. Staying Honest at the Edges",
    "title": "8. Staying Honest at the Edges",
    "content": "Coming soon… . ",
    "url": "/fallacytag/pages/08-constraints/",
    
    "relUrl": "/pages/08-constraints/"
  },"16": {
    "doc": "9. Conditional Case - When This Is Worth Building",
    "title": "9. Conditional Case: When This Is Worth Building",
    "content": "Coming soon… . ",
    "url": "/fallacytag/pages/09-worth-building/#9-conditional-case-when-this-is-worth-building",
    
    "relUrl": "/pages/09-worth-building/#9-conditional-case-when-this-is-worth-building"
  },"17": {
    "doc": "9. Conditional Case - When This Is Worth Building",
    "title": "9. Conditional Case - When This Is Worth Building",
    "content": " ",
    "url": "/fallacytag/pages/09-worth-building/",
    
    "relUrl": "/pages/09-worth-building/"
  },"18": {
    "doc": "10. Call to Explore",
    "title": "10. Call to Explore",
    "content": "Coming soon… . ",
    "url": "/fallacytag/pages/10-call-to-explore/",
    
    "relUrl": "/pages/10-call-to-explore/"
  },"19": {
    "doc": "Appendix A - Logic Check Experiment: Why Democracy Is Mathematically Impossible by Veritasium",
    "title": "Appendix A - Logic Check Experiment: Why Democracy Is Mathematically Impossible by Veritasium",
    "content": " ",
    "url": "/fallacytag/pages/appendices/appendix-a/",
    
    "relUrl": "/pages/appendices/appendix-a/"
  },"20": {
    "doc": "Appendix A - Logic Check Experiment: Why Democracy Is Mathematically Impossible by Veritasium",
    "title": "Table of Contents",
    "content": ". | Appendix A - Logic Check Experiment: Why Democracy Is Mathematically Impossible by Veritasium . | Source | Case Study: (Inadvertently) Scaffolded Prompt . | 🔍 Prompt | 🤖 GPT Response | . | . | . Source . https://youtu.be/qf7ws2DF-zk . Case Study: (Inadvertently) Scaffolded Prompt . 🔍 Prompt . I’d like to discuss the logical quality of the conclusion of a YouTube transcript. This is the core of the argument, without conclusion: . … (Ommitted for brevity; transcript remains available on Youtube) … . Now the conclusion: . “So is democracy mathematically impossible? Well, yes, if we use ranked choice methods of voting, which is what most countries in the world use to elect their leaders. And some methods are clearly better at aggregating the people’s preferences than others, the use of first past the post voting feels quite frankly ridiculous to me, given all of its flaws. But just because things aren’t perfect doesn’t mean we shouldn’t try. Being interested in the world around us, caring about issues, and being politically engaged is important. It might be one of the few ways we can make a real difference in the world. Like Winston Churchill said, ‘Democracy is the worst form of government except for all the other forms that have been tried.’ Democracy is not perfect, but it’s the best thing we’ve got. The game might be crooked, but it’s the only game in town.” . Does the conclusion actually follow from the previous content? Is it logical? Do you detect specific fallacies? . 🤖 GPT Response . The unabridged response is lost in the ether, but the exact content that impressed me, manually organized in a single table (copy/paste only), follows: . | Fallacy Name | Explanation in Context of the Transcript Conclusion | . | Equivocation | The word “democracy” is used in two different senses: (1) a political system of collective decision-making, and (2) a voting method that aggregates individual ranked preferences. The conclusion claims “democracy is mathematically impossible” based on the second meaning, while implying a broader political critique using the first. | . | Category Error | Arrow’s theorem demonstrates limits of preference aggregation mechanisms, not of democracy as a political institution. The conclusion incorrectly treats a limitation in one technical component as a flaw in the overall concept. | . | Appeal to Authority / Sentiment | The Churchill quote (“Democracy is the worst form…”) is invoked to justify staying engaged with democracy, but it bypasses logical argumentation. Churchill is not a mathematical authority, and his aphorism is used to close a technical argument emotionally, not analytically. | . | Non Sequitur | The conclusion jumps from “perfect preference aggregation is impossible” to “therefore, we should stay politically engaged.” This leap is not logically justified by the body of the argument, which focused narrowly on voting system mechanics, not civic virtue or political participation. | . ",
    "url": "/fallacytag/pages/appendices/appendix-a/#table-of-contents",
    
    "relUrl": "/pages/appendices/appendix-a/#table-of-contents"
  },"21": {
    "doc": "Appendix B - Logic Check Experiment: Reddit CMV on Local vs National Politics",
    "title": "Appendix B - Logic Check Experiment: Reddit CMV on Local vs National Politics",
    "content": " ",
    "url": "/fallacytag/pages/appendices/appendix-b/",
    
    "relUrl": "/pages/appendices/appendix-b/"
  },"22": {
    "doc": "Appendix B - Logic Check Experiment: Reddit CMV on Local vs National Politics",
    "title": "Table of Contents",
    "content": ". | Appendix B - Logic Check Experiment: Reddit CMV on Local vs National Politics . | Source | 🔍 Prompt (used in all three cases) | Case 1 . | 💬 Comment | 🤖 GPT Response | . | Case 2 . | 💬 Comment | 🤖 GPT Response | . | Case 3 . | 💬 Comment | 🤖 GPT Response | . | . | . Source . r/ChangeMyView CMV: Local politics is much more important to your daily life than national politics . 🔍 Prompt (used in all three cases) . Identify any flawed reasoning and explain why: . (comment text) . Case 1 . 💬 Comment . Would you say the difference between two different towns of the same size in your state is more or less significant than the difference between your town and the most similar Canadian analog? Because it seems to me that one town’s very like another when your head’s down over your pieces, brother whereas countries are much more strongly different from one another. Because national politics really varies much more sharply. In theory local politics could vary more sharply but in practice it doesn’t because towns and cities mostly just copy one another and if they do try to do crazy stuff people move away. 🤖 GPT Response . This passage contains several instances of flawed reasoning, particularly in terms of overgeneralization, false equivalence, and unsupported assumptions. Here’s a breakdown: . 1. Overgeneralization: . “Towns and cities mostly just copy one another…” . | This is an overgeneralization. While towns may adopt similar policies due to shared challenges, many diverge meaningfully in leadership, culture, and priorities. | . 2. False Dichotomy / False Equivalence: . “One town’s very like another… whereas countries are much more strongly different…” . | The comparison oversimplifies. Some Canadian towns may resemble American towns more than other American towns do. | . 3. Unsupported Causal Claim: . “…if they do try to do crazy stuff people move away.” . | People move for many reasons; the claim assumes too direct a relationship between local policy and migration. | . 4. Appeal to Subjective Experience: . “…when your head’s down over your pieces, brother…” . | Rhetorically colorful, but relies on personal sentiment rather than logic or data. | . Conclusion: The reasoning overstates national variation and downplays meaningful local differences through generalizations and untested assumptions. Case 2 . 💬 Comment . Local politics define if there’s some bollards or speed bumps on my street. National politics decide if I have healthcare, how much I pay for university and if I can buy a house someday or not. 🤖 GPT Response . 💡 . The statement draws a reasonable contrast but contains oversimplifications and false dichotomy. Key issues include: . 1. False Dichotomy / Oversimplification: . | It implies only federal politics matter for big issues. In fact: | Housing affordability is heavily influenced by local zoning. | University costs often depend on state funding. | Healthcare access is shaped by both federal and state policy. | . 2. Implied Value Judgment: . | It subtly dismisses local politics as trivial (“speed bumps”), ignoring important local functions like schooling, transit, and public safety. | . 3. Selective Framing: . | The examples cherry-pick federal power while ignoring where local control is dominant. | . Conclusion: While the federal government shapes many broad systems, the argument overstates its exclusivity and underrepresents local impact on life outcomes. Case 3 . 💬 Comment . No one’s ever been drafted by their state or county government. Federal taxes are much higher than local. Teaching to the test is exacerbated by W’s No Child Left Behind bill. Many local road projects are majority funded by federal funds, even more so for highways. Almost everyone will come to depend on Medicare and Social Security at some point in their lives. 🤖 GPT Response . The comment builds a case for federal dominance in influence, but relies on selective evidence, scope neglect, and equivocation. 1. Cherry-Picking: . | It highlights federal powers (e.g., draft, taxes, entitlements) while ignoring where state/local governments are deeply consequential (e.g., police, schools, housing). | . 2. Scope Neglect: . “Federal taxes are much higher…” . | Often true, but ignores that local taxes fund vital, visible services (schools, water, fire). | . 3. Oversimplification: . “Teaching to the test is exacerbated by No Child Left Behind…” . | True to an extent, but the law was interpreted and implemented differently by each state, and local schools determined how to meet test mandates. | . 4. Equivocation on ‘importance’: . | It conflates scale of policy with impact on daily life. For many, local decisions (e.g., crime, housing, school quality) are just as impactful as national ones. | . Conclusion: The argument rests on a one-sided portrayal of federal authority, minimizing the real power and variability of local governance. ",
    "url": "/fallacytag/pages/appendices/appendix-b/#table-of-contents",
    
    "relUrl": "/pages/appendices/appendix-b/#table-of-contents"
  },"23": {
    "doc": "Appendix C: On Learning from User Feedback",
    "title": "Appendix C: On Learning from User Feedback",
    "content": "B.1 The Intuition . FallacyTag invites feedback. When a user marks a tag as “incorrect,” that judgment is recorded. In theory, such input could help the system improve—by refining detection strategies, adjusting thresholds, or adapting to context over time. The intuition is familiar: learn from disagreement. But in practice, learning from this kind of feedback is much harder than it looks. B.2 Why Feedback Is Ambiguous . Not all rejections are equal—and not all are about reasoning. A user might dismiss a tag because: . | It feels wrong for the tone or genre (e.g., casual podcast vs. formal debate) | They don’t like the interruption | They agree with the conclusion, and mistake critique for contradiction | . In reasoning contexts, the line between “this is a fallacy” and “this rubs me the wrong way” is thin. Even sincere feedback can mislead if the signal is entangled with sentiment, ideology, or attention mode. B.3 Social Bias and Tribal Rejection . Beyond ambiguity, there’s distortion. In public contexts, feedback is shaped by: . | Confirmation bias — users tend to protect reasoning they agree with | Social signaling — rejections often reflect tribal alignment, not logical appraisal | Moral loading — tags may be read as ideological attacks, not structural cues | . In polarized environments, these effects can dominate. Tags get rejected not because they’re wrong, but because they’re unwelcome. In such settings, FallacyTag may not be misfiring—it may simply be misfit. B.4 Strategies Considered (and Their Limits) . We explored multiple feedback-driven improvement strategies: . | Strategy | Idea | Limitation | . | Rejection Clustering | Flag tags that get dismissed often | May reinforce local bias or majority opinion | . | Contextual Caching | Tune logic by media format or genre | Requires strong priors; risks overfitting | . | Prompt Chain Feedback | Feed rejections back into generation chains | Adds complexity without clear gains | . | Revision Tracing | Look for edits following dismissals | Sparse and difficult to correlate with feedback meaningfully | . | Cohort Modeling | Segment users by behavior for weighting | Requires scale and raises privacy/framing questions | . Each approach offered a partial signal. None yielded a robust, general-purpose improvement loop. B.5 Why the Loop Is Open—For Now . FallacyTag currently stores feedback but does not act on it. This is intentional. In a system designed to support reasoning—not just user preference—learning from ambiguous or adversarial signals may do more harm than good. The system might shift toward comfort, not clarity. The right path may involve: . | Better modeling of user intent | New interaction patterns that disambiguate feedback | Feedback that arises from editing behavior, not just button taps | . Until then, restraint is a feature, not a limitation. The system listens—but it does not contort. ",
    "url": "/fallacytag/pages/appendices/appendix-c/",
    
    "relUrl": "/pages/appendices/appendix-c/"
  },"24": {
    "doc": "Appendix D: FallacyTag Variability Model",
    "title": "Appendix D: FallacyTag Variability Model",
    "content": " ",
    "url": "/fallacytag/pages/appendices/appendix-d/",
    
    "relUrl": "/pages/appendices/appendix-d/"
  },"25": {
    "doc": "Appendix D: FallacyTag Variability Model",
    "title": "Table of Contents",
    "content": ". | 🔧 Variation Points Summary Table | 🧭 Feature Hierarchy Diagram | 📌 Usage | . This appendix presents a structured view of the FallacyTag architecture as a product line architecture (PLA), capturing its key variation points, available options, and constraints. It supports implementers in identifying viable configurations and understanding architectural trade-offs. 🔧 Variation Points Summary Table . | Variation Point | Options | Constraints | Notes | . | Privacy Handling (Privacy ↔ Observability) | Local-only, Session-based, Identity-linked | Must preserve user autonomy; no profiling; minimal friction | Options vary in how much data persists and travels: “local-only” retains nothing, “identity-linked” enables long-term telemetry, requiring strong safeguards. | . | Inference Type (Statelessness ↔ Adaptivity) | Prompt-only, Session-adaptive, Fine-tuned w/ memory | Must preserve auditability and avoid unpredictable drift | “Prompt-only” provides maximal transparency. “Fine-tuned” risks subtle shifts. Session-adaptive sits in between, retaining state temporarily. | . | Tag Strategy (Interpretability ↔ Expressive Power) | Symbolic (sentence-level), Discourse-aware, Learned patterns | Must maintain interpretability; minimize hallucination | “Symbolic” uses explicit templates and rule-based mappings. “Learned patterns” are more flexible but harder to explain. | . | UI Integration (Separation ↔ Intrusion) | Post-hoc Review, Inline Overlay, Export-Only | Must preserve primary experience flow; avoid disruptive or adversarial UX | “Post-hoc” works best in slower or asynchronous modes; “inline” suits text; “export-only” fits detached workflows like podcasts. | . | Feedback Use | None, Aggregate Telemetry, User-linked Adaptation | Feedback must not affect inference by default; linkage to identity must be explicitly authorized | Intersects both privacy and adaptivity: feedback pipelines can erode statelessness and compromise anonymity. Aggregate telemetry enables improvement without retraining or profiling. | . | Downstream Use (Reflection ↔ Enforcement Drift) | Reflective tagging, Optional annotations, Scoring/moderation | Must avoid authoritative or punitive use; system intent must remain transparent | Tags may be reused for judgment or ranking downstream, even if designed for reflection. Implementers must guard against misinterpretation of system posture. | . 🧭 Feature Hierarchy Diagram . 📌 Usage . This model provides implementers and researchers with a structured way to reason about system variants. It complements the prose discussion in Section 6.2 and is suitable for: . | Evaluating architectural fit for a target domain (e.g., public discourse vs. classrooms) | Documenting design decisions and their implications | Supporting modular implementation planning | . Future work may include mapping these features to implementation modules or tracing variation point dependencies. ",
    "url": "/fallacytag/pages/appendices/appendix-d/#table-of-contents",
    
    "relUrl": "/pages/appendices/appendix-d/#table-of-contents"
  },"26": {
    "doc": "Appendices",
    "title": "Appendices",
    "content": "This section collects supplementary material, extended analysis, and design reflections referenced throughout the main text. While not required for core understanding, these appendices offer deeper insight into decisions made during the development of FallacyTag. | Appendix A is the transcript of GPT-4’s logical check of Veritassium’s “Why Democracy Is Mathematically Impossible” video. | Appendix B is the transcript of GPT-4’s logical check of a few posts on Reddit CMV on Local vs National Politics. | Appendix C discusses the challenges and limitations of using user feedback to improve the system. | Appendix D presents a structured view of the FallacyTag architecture as a product line architecture (PLA) | . ",
    "url": "/fallacytag/pages/appendices/appendix-home/",
    
    "relUrl": "/pages/appendices/appendix-home/"
  },"27": {
    "doc": "References",
    "title": "References",
    "content": "Coming soon… . ",
    "url": "/fallacytag/pages/references/",
    
    "relUrl": "/pages/references/"
  }
}
